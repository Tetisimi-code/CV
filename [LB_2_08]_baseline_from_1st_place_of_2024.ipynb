{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaL4",
      "dataSources": [
        {
          "sourceId": 91496,
          "databundleVersionId": 11483707,
          "sourceType": "competition"
        },
        {
          "sourceId": 9515958,
          "sourceType": "datasetVersion",
          "datasetId": 5793177
        },
        {
          "sourceId": 158171,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 134422,
          "modelId": 157175
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "[LB 2.08] baseline from 1st place of 2024",
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8cab9664d5c4a71807895cdbf3e75ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6c70bd317194642a08fdf75244a9d2b"
            ],
            "layout": "IPY_MODEL_ff114d5dd7dc4ca288125f4f70fee8de"
          }
        },
        "9129209f34824db2b49a63fd0a8f78b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d25c5fefbcb64ff591f601496a95315f",
            "placeholder": "​",
            "style": "IPY_MODEL_044b086ebcf84bc0a4c9c95d99c37710",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "5b1c68de995a4c8582ce0a37aaa91d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f4c5143859254aed807152612758a15a",
            "placeholder": "​",
            "style": "IPY_MODEL_335693d938294b21b7be3d06a09f5b3d",
            "value": "tetisimi"
          }
        },
        "e91844e850e845cca66a9f8c8181a968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c8b5d698575c40c4920e695e6ccd93b0",
            "placeholder": "​",
            "style": "IPY_MODEL_66dc6ee580d642a3b783895fba87f290",
            "value": ""
          }
        },
        "e87e8b31fcf040bb9106fcb16fd22124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_05d25be46655415ab27c8fa7406729ec",
            "style": "IPY_MODEL_305709cda0d64224bb442a2383eab2a5",
            "tooltip": ""
          }
        },
        "8798636ee6bf46faa18252307c379d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0036e957941a4275b57fcd5b73a88020",
            "placeholder": "​",
            "style": "IPY_MODEL_f03ee5679f09429f8e0639f49590ca85",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "ff114d5dd7dc4ca288125f4f70fee8de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d25c5fefbcb64ff591f601496a95315f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "044b086ebcf84bc0a4c9c95d99c37710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4c5143859254aed807152612758a15a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "335693d938294b21b7be3d06a09f5b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8b5d698575c40c4920e695e6ccd93b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66dc6ee580d642a3b783895fba87f290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05d25be46655415ab27c8fa7406729ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305709cda0d64224bb442a2383eab2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0036e957941a4275b57fcd5b73a88020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03ee5679f09429f8e0639f49590ca85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "685f4239c138417eb2a4998dd3765cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25c9c08bdc84434289064c25ba228738",
            "placeholder": "​",
            "style": "IPY_MODEL_1c60e9a2111449dc87832df91a894935",
            "value": "Connecting..."
          }
        },
        "25c9c08bdc84434289064c25ba228738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c60e9a2111449dc87832df91a894935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6c70bd317194642a08fdf75244a9d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2d51da6dcaf462e9bf1748839847c72",
            "placeholder": "​",
            "style": "IPY_MODEL_25a907dfe9e24a35a292407aae5273e7",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "b2d51da6dcaf462e9bf1748839847c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a907dfe9e24a35a292407aae5273e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b3765284f1a4165b31c11bd8d5e0814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d639220c2c44c96b44051ed76895ba6",
              "IPY_MODEL_c0625e9b39f5479285903c3ffa4852fc",
              "IPY_MODEL_d1b50458a6ae4a6586a9530529edbb79"
            ],
            "layout": "IPY_MODEL_b73ea78f36b64b99aa6ce066b9ad657f"
          }
        },
        "8d639220c2c44c96b44051ed76895ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c01c8907733049f8a0d4ff5039a9fabb",
            "placeholder": "​",
            "style": "IPY_MODEL_8f1f65eead094a96929ec90ce316d66b",
            "value": "Downloading 6 files: 100%"
          }
        },
        "c0625e9b39f5479285903c3ffa4852fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53af3f7cd97a47e6b60febdf7be3c63b",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45c601d4801a45599921dabd71c95321",
            "value": 6
          }
        },
        "d1b50458a6ae4a6586a9530529edbb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be2c32f37a12440eb0e312e3c7d71fef",
            "placeholder": "​",
            "style": "IPY_MODEL_7850a805a41b4156ac449265b8c16936",
            "value": " 6/6 [00:47&lt;00:00, 17.36s/it]"
          }
        },
        "b73ea78f36b64b99aa6ce066b9ad657f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01c8907733049f8a0d4ff5039a9fabb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f1f65eead094a96929ec90ce316d66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53af3f7cd97a47e6b60febdf7be3c63b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45c601d4801a45599921dabd71c95321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be2c32f37a12440eb0e312e3c7d71fef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7850a805a41b4156ac449265b8c16936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tetisimi-code/CV/blob/main/%5BLB_2_08%5D_baseline_from_1st_place_of_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "f8cab9664d5c4a71807895cdbf3e75ca",
            "9129209f34824db2b49a63fd0a8f78b2",
            "5b1c68de995a4c8582ce0a37aaa91d9a",
            "e91844e850e845cca66a9f8c8181a968",
            "e87e8b31fcf040bb9106fcb16fd22124",
            "8798636ee6bf46faa18252307c379d58",
            "ff114d5dd7dc4ca288125f4f70fee8de",
            "d25c5fefbcb64ff591f601496a95315f",
            "044b086ebcf84bc0a4c9c95d99c37710",
            "f4c5143859254aed807152612758a15a",
            "335693d938294b21b7be3d06a09f5b3d",
            "c8b5d698575c40c4920e695e6ccd93b0",
            "66dc6ee580d642a3b783895fba87f290",
            "05d25be46655415ab27c8fa7406729ec",
            "305709cda0d64224bb442a2383eab2a5",
            "0036e957941a4275b57fcd5b73a88020",
            "f03ee5679f09429f8e0639f49590ca85",
            "685f4239c138417eb2a4998dd3765cc9",
            "25c9c08bdc84434289064c25ba228738",
            "1c60e9a2111449dc87832df91a894935",
            "e6c70bd317194642a08fdf75244a9d2b",
            "b2d51da6dcaf462e9bf1748839847c72",
            "25a907dfe9e24a35a292407aae5273e7"
          ]
        },
        "id": "c92xL6FqMBWh",
        "outputId": "711d7d36-5d44-4b33-b2c0-0b696632833d"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8cab9664d5c4a71807895cdbf3e75ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "arc_prize_2025_path = kagglehub.competition_download('arc-prize-2025')\n",
        "dfranzen_unsloth_2024_9_post4_path = kagglehub.dataset_download('dfranzen/unsloth-2024-9-post4')\n",
        "dfranzen_wb55l_nemomini_fulleval_transformers_default_1_path = kagglehub.model_download('dfranzen/wb55l_nemomini_fulleval/Transformers/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5b3765284f1a4165b31c11bd8d5e0814",
            "8d639220c2c44c96b44051ed76895ba6",
            "c0625e9b39f5479285903c3ffa4852fc",
            "d1b50458a6ae4a6586a9530529edbb79",
            "b73ea78f36b64b99aa6ce066b9ad657f",
            "c01c8907733049f8a0d4ff5039a9fabb",
            "8f1f65eead094a96929ec90ce316d66b",
            "53af3f7cd97a47e6b60febdf7be3c63b",
            "45c601d4801a45599921dabd71c95321",
            "be2c32f37a12440eb0e312e3c7d71fef",
            "7850a805a41b4156ac449265b8c16936"
          ]
        },
        "id": "cNa53PLzMBWi",
        "outputId": "7528a0f6-fe93-43f7-8d29-040ec41e30b9"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/competitions/data/download-all/arc-prize-2025...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 487k/487k [00:00<00:00, 86.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dfranzen/unsloth-2024-9-post4?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.84G/2.84G [00:35<00:00, 85.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b3765284f1a4165b31c11bd8d5e0814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/dfranzen/wb55l_nemomini_fulleval/Transformers/default/1/download/generation_config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 154/154 [00:00<00:00, 333kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/dfranzen/wb55l_nemomini_fulleval/Transformers/default/1/download/tokenizer_config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/dfranzen/wb55l_nemomini_fulleval/Transformers/default/1/download/config.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0.00/0.99k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "100%|██████████| 1.16k/1.16k [00:00<00:00, 1.81MB/s]\n",
            "100%|██████████| 0.99k/0.99k [00:00<00:00, 181kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/dfranzen/wb55l_nemomini_fulleval/Transformers/default/1/download/tokenizer.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 4.00k/4.00k [00:00<00:00, 4.04MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/dfranzen/wb55l_nemomini_fulleval/Transformers/default/1/download/special_tokens_map.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 438/438 [00:00<00:00, 808kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/dfranzen/wb55l_nemomini_fulleval/Transformers/default/1/download/model.safetensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0.00/3.53G [00:00<?, ?B/s]\u001b[A\n",
            "  0%|          | 1.00M/3.53G [00:00<11:55, 5.30MB/s]\u001b[A\n",
            "  0%|          | 8.00M/3.53G [00:00<01:52, 33.6MB/s]\u001b[A\n",
            "  0%|          | 17.0M/3.53G [00:00<01:13, 51.1MB/s]\u001b[A\n",
            "  1%|          | 23.0M/3.53G [00:00<01:28, 42.5MB/s]\u001b[A\n",
            "  1%|          | 29.0M/3.53G [00:00<01:30, 41.3MB/s]\u001b[A\n",
            "  1%|          | 36.0M/3.53G [00:00<01:30, 41.5MB/s]\u001b[A\n",
            "  1%|          | 43.0M/3.53G [00:01<01:18, 48.0MB/s]\u001b[A\n",
            "  1%|▏         | 53.0M/3.53G [00:01<01:01, 60.4MB/s]\u001b[A\n",
            "  2%|▏         | 60.0M/3.53G [00:01<01:07, 55.6MB/s]\u001b[A\n",
            "  2%|▏         | 70.0M/3.53G [00:01<00:55, 67.3MB/s]\u001b[A\n",
            "  2%|▏         | 78.0M/3.53G [00:01<01:00, 60.9MB/s]\u001b[A\n",
            "  2%|▏         | 86.0M/3.53G [00:01<01:04, 57.7MB/s]\u001b[A\n",
            "  3%|▎         | 93.0M/3.53G [00:01<01:03, 57.8MB/s]\u001b[A\n",
            "  3%|▎         | 100M/3.53G [00:01<01:00, 61.1MB/s] \u001b[A\n",
            "  3%|▎         | 107M/3.53G [00:02<01:00, 61.0MB/s]\u001b[A\n",
            "  3%|▎         | 117M/3.53G [00:02<00:52, 69.4MB/s]\u001b[A\n",
            "  4%|▎         | 128M/3.53G [00:02<00:45, 80.5MB/s]\u001b[A\n",
            "  4%|▍         | 136M/3.53G [00:02<00:54, 66.6MB/s]\u001b[A\n",
            "  4%|▍         | 146M/3.53G [00:02<00:48, 75.6MB/s]\u001b[A\n",
            "  4%|▍         | 158M/3.53G [00:02<00:41, 87.5MB/s]\u001b[A\n",
            "  5%|▍         | 169M/3.53G [00:02<00:38, 93.6MB/s]\u001b[A\n",
            "  5%|▍         | 180M/3.53G [00:02<00:37, 95.4MB/s]\u001b[A\n",
            "  5%|▌         | 192M/3.53G [00:03<00:35, 102MB/s] \u001b[A\n",
            "  6%|▌         | 204M/3.53G [00:03<00:35, 102MB/s]\u001b[A\n",
            "  6%|▌         | 215M/3.53G [00:03<00:33, 105MB/s]\u001b[A\n",
            "  6%|▋         | 226M/3.53G [00:03<00:33, 106MB/s]\u001b[A\n",
            "  7%|▋         | 237M/3.53G [00:03<00:34, 102MB/s]\u001b[A\n",
            "  7%|▋         | 247M/3.53G [00:03<00:35, 101MB/s]\u001b[A\n",
            "  7%|▋         | 258M/3.53G [00:03<00:34, 101MB/s]\u001b[A\n",
            "  7%|▋         | 268M/3.53G [00:03<00:34, 100MB/s]\u001b[A\n",
            "  8%|▊         | 281M/3.53G [00:03<00:32, 109MB/s]\u001b[A\n",
            "  8%|▊         | 292M/3.53G [00:04<00:31, 110MB/s]\u001b[A\n",
            "  8%|▊         | 303M/3.53G [00:04<00:33, 104MB/s]\u001b[A\n",
            "  9%|▊         | 314M/3.53G [00:04<00:33, 103MB/s]\u001b[A\n",
            "  9%|▉         | 325M/3.53G [00:04<00:32, 106MB/s]\u001b[A\n",
            "  9%|▉         | 336M/3.53G [00:04<00:33, 102MB/s]\u001b[A\n",
            " 10%|▉         | 348M/3.53G [00:04<00:33, 103MB/s]\u001b[A\n",
            " 10%|▉         | 360M/3.53G [00:04<00:32, 106MB/s]\u001b[A\n",
            " 10%|█         | 371M/3.53G [00:04<00:32, 104MB/s]\u001b[A\n",
            " 11%|█         | 381M/3.53G [00:04<00:33, 103MB/s]\u001b[A\n",
            " 11%|█         | 391M/3.53G [00:05<00:38, 87.0MB/s]\u001b[A\n",
            " 11%|█         | 401M/3.53G [00:05<00:37, 90.4MB/s]\u001b[A\n",
            " 11%|█▏        | 410M/3.53G [00:05<00:40, 82.1MB/s]\u001b[A\n",
            " 12%|█▏        | 421M/3.53G [00:05<00:37, 90.3MB/s]\u001b[A\n",
            " 12%|█▏        | 432M/3.53G [00:05<00:35, 94.9MB/s]\u001b[A\n",
            " 12%|█▏        | 442M/3.53G [00:05<00:39, 84.5MB/s]\u001b[A\n",
            " 12%|█▏        | 451M/3.53G [00:05<00:47, 69.7MB/s]\u001b[A\n",
            " 13%|█▎        | 461M/3.53G [00:06<00:42, 77.0MB/s]\u001b[A\n",
            " 13%|█▎        | 474M/3.53G [00:06<00:36, 89.3MB/s]\u001b[A\n",
            " 13%|█▎        | 484M/3.53G [00:06<00:37, 86.7MB/s]\u001b[A\n",
            " 14%|█▎        | 493M/3.53G [00:06<00:41, 78.8MB/s]\u001b[A\n",
            " 14%|█▍        | 502M/3.53G [00:06<00:41, 79.0MB/s]\u001b[A\n",
            " 14%|█▍        | 510M/3.53G [00:06<00:43, 75.7MB/s]\u001b[A\n",
            " 14%|█▍        | 518M/3.53G [00:06<00:49, 65.7MB/s]\u001b[A\n",
            " 15%|█▍        | 525M/3.53G [00:06<00:52, 62.2MB/s]\u001b[A\n",
            " 15%|█▍        | 532M/3.53G [00:07<00:52, 61.5MB/s]\u001b[A\n",
            " 15%|█▍        | 539M/3.53G [00:07<00:52, 61.7MB/s]\u001b[A\n",
            " 15%|█▌        | 547M/3.53G [00:07<00:48, 67.0MB/s]\u001b[A\n",
            " 15%|█▌        | 555M/3.53G [00:07<00:47, 66.9MB/s]\u001b[A\n",
            " 16%|█▌        | 562M/3.53G [00:07<00:49, 65.2MB/s]\u001b[A\n",
            " 16%|█▌        | 571M/3.53G [00:07<00:44, 72.0MB/s]\u001b[A\n",
            " 16%|█▌        | 578M/3.53G [00:07<00:46, 68.3MB/s]\u001b[A\n",
            " 16%|█▌        | 585M/3.53G [00:07<00:47, 67.6MB/s]\u001b[A\n",
            " 16%|█▋        | 593M/3.53G [00:08<00:44, 70.8MB/s]\u001b[A\n",
            " 17%|█▋        | 602M/3.53G [00:08<00:41, 76.6MB/s]\u001b[A\n",
            " 17%|█▋        | 611M/3.53G [00:08<00:39, 80.7MB/s]\u001b[A\n",
            " 17%|█▋        | 620M/3.53G [00:08<00:37, 83.5MB/s]\u001b[A\n",
            " 17%|█▋        | 629M/3.53G [00:08<00:36, 86.0MB/s]\u001b[A\n",
            " 18%|█▊        | 640M/3.53G [00:08<00:33, 93.9MB/s]\u001b[A\n",
            " 18%|█▊        | 650M/3.53G [00:08<00:40, 76.5MB/s]\u001b[A\n",
            " 18%|█▊        | 658M/3.53G [00:08<00:40, 76.1MB/s]\u001b[A\n",
            " 18%|█▊        | 666M/3.53G [00:08<00:42, 72.6MB/s]\u001b[A\n",
            " 19%|█▉        | 678M/3.53G [00:09<00:36, 83.6MB/s]\u001b[A\n",
            " 19%|█▉        | 687M/3.53G [00:09<00:38, 79.7MB/s]\u001b[A\n",
            " 19%|█▉        | 695M/3.53G [00:09<00:39, 77.3MB/s]\u001b[A\n",
            " 19%|█▉        | 703M/3.53G [00:09<00:39, 76.8MB/s]\u001b[A\n",
            " 20%|█▉        | 716M/3.53G [00:09<00:33, 90.6MB/s]\u001b[A\n",
            " 20%|██        | 729M/3.53G [00:09<00:29, 102MB/s] \u001b[A\n",
            " 20%|██        | 741M/3.53G [00:09<00:28, 106MB/s]\u001b[A\n",
            " 21%|██        | 752M/3.53G [00:09<00:27, 108MB/s]\u001b[A\n",
            " 21%|██        | 763M/3.53G [00:09<00:27, 110MB/s]\u001b[A\n",
            " 21%|██▏       | 774M/3.53G [00:10<00:35, 85.0MB/s]\u001b[A\n",
            " 22%|██▏       | 783M/3.53G [00:10<00:41, 71.3MB/s]\u001b[A\n",
            " 22%|██▏       | 791M/3.53G [00:10<00:42, 68.9MB/s]\u001b[A\n",
            " 22%|██▏       | 799M/3.53G [00:10<00:48, 60.9MB/s]\u001b[A\n",
            " 22%|██▏       | 806M/3.53G [00:10<00:48, 60.9MB/s]\u001b[A\n",
            " 22%|██▏       | 813M/3.53G [00:10<00:46, 63.3MB/s]\u001b[A\n",
            " 23%|██▎       | 822M/3.53G [00:11<00:41, 70.8MB/s]\u001b[A\n",
            " 23%|██▎       | 838M/3.53G [00:11<00:30, 95.2MB/s]\u001b[A\n",
            " 23%|██▎       | 848M/3.53G [00:11<00:39, 74.2MB/s]\u001b[A\n",
            " 24%|██▎       | 857M/3.53G [00:11<00:37, 77.5MB/s]\u001b[A\n",
            " 24%|██▍       | 866M/3.53G [00:11<00:37, 77.5MB/s]\u001b[A\n",
            " 24%|██▍       | 875M/3.53G [00:11<00:35, 81.1MB/s]\u001b[A\n",
            " 25%|██▍       | 886M/3.53G [00:11<00:32, 89.3MB/s]\u001b[A\n",
            " 25%|██▍       | 897M/3.53G [00:11<00:29, 96.1MB/s]\u001b[A\n",
            " 25%|██▌       | 907M/3.53G [00:12<00:31, 88.8MB/s]\u001b[A\n",
            " 25%|██▌       | 920M/3.53G [00:12<00:28, 100MB/s] \u001b[A\n",
            " 26%|██▌       | 930M/3.53G [00:12<00:29, 94.7MB/s]\u001b[A\n",
            " 26%|██▌       | 940M/3.53G [00:12<00:31, 89.8MB/s]\u001b[A\n",
            " 26%|██▌       | 949M/3.53G [00:12<00:44, 63.0MB/s]\u001b[A\n",
            " 26%|██▋       | 957M/3.53G [00:12<00:54, 51.1MB/s]\u001b[A\n",
            " 27%|██▋       | 963M/3.53G [00:13<00:59, 47.0MB/s]\u001b[A\n",
            " 27%|██▋       | 969M/3.53G [00:13<01:02, 44.5MB/s]\u001b[A\n",
            " 27%|██▋       | 974M/3.53G [00:13<01:01, 45.0MB/s]\u001b[A\n",
            " 27%|██▋       | 985M/3.53G [00:13<00:46, 59.3MB/s]\u001b[A\n",
            " 28%|██▊       | 997M/3.53G [00:13<00:37, 72.3MB/s]\u001b[A\n",
            " 28%|██▊       | 0.98G/3.53G [00:13<00:33, 82.4MB/s]\u001b[A\n",
            " 28%|██▊       | 1.00G/3.53G [00:13<00:28, 95.5MB/s]\u001b[A\n",
            " 29%|██▊       | 1.01G/3.53G [00:13<00:27, 98.5MB/s]\u001b[A\n",
            " 29%|██▉       | 1.02G/3.53G [00:14<00:33, 81.5MB/s]\u001b[A\n",
            " 29%|██▉       | 1.03G/3.53G [00:14<00:34, 78.4MB/s]\u001b[A\n",
            " 29%|██▉       | 1.04G/3.53G [00:14<00:29, 91.6MB/s]\u001b[A\n",
            " 30%|██▉       | 1.05G/3.53G [00:14<00:30, 87.8MB/s]\u001b[A\n",
            " 30%|██▉       | 1.06G/3.53G [00:14<00:30, 86.6MB/s]\u001b[A\n",
            " 30%|███       | 1.07G/3.53G [00:14<00:28, 91.6MB/s]\u001b[A\n",
            " 31%|███       | 1.08G/3.53G [00:14<00:26, 100MB/s] \u001b[A\n",
            " 31%|███       | 1.09G/3.53G [00:14<00:25, 104MB/s]\u001b[A\n",
            " 31%|███▏      | 1.10G/3.53G [00:15<00:24, 105MB/s]\u001b[A\n",
            " 32%|███▏      | 1.11G/3.53G [00:15<00:24, 108MB/s]\u001b[A\n",
            " 32%|███▏      | 1.13G/3.53G [00:15<00:23, 112MB/s]\u001b[A\n",
            " 32%|███▏      | 1.14G/3.53G [00:15<00:22, 112MB/s]\u001b[A\n",
            " 33%|███▎      | 1.15G/3.53G [00:15<00:23, 109MB/s]\u001b[A\n",
            " 33%|███▎      | 1.16G/3.53G [00:15<00:23, 107MB/s]\u001b[A\n",
            " 33%|███▎      | 1.17G/3.53G [00:15<00:22, 114MB/s]\u001b[A\n",
            " 33%|███▎      | 1.18G/3.53G [00:15<00:22, 112MB/s]\u001b[A\n",
            " 34%|███▍      | 1.19G/3.53G [00:15<00:22, 114MB/s]\u001b[A\n",
            " 34%|███▍      | 1.21G/3.53G [00:15<00:24, 102MB/s]\u001b[A\n",
            " 34%|███▍      | 1.22G/3.53G [00:16<00:23, 105MB/s]\u001b[A\n",
            " 35%|███▍      | 1.23G/3.53G [00:16<00:22, 108MB/s]\u001b[A\n",
            " 35%|███▌      | 1.24G/3.53G [00:16<00:25, 97.5MB/s]\u001b[A\n",
            " 35%|███▌      | 1.25G/3.53G [00:16<00:26, 92.1MB/s]\u001b[A\n",
            " 36%|███▌      | 1.26G/3.53G [00:17<01:12, 33.8MB/s]\u001b[A\n",
            " 36%|███▌      | 1.27G/3.53G [00:17<00:51, 47.0MB/s]\u001b[A\n",
            " 36%|███▋      | 1.28G/3.53G [00:17<00:42, 56.8MB/s]\u001b[A\n",
            " 37%|███▋      | 1.29G/3.53G [00:17<00:35, 68.1MB/s]\u001b[A\n",
            " 37%|███▋      | 1.30G/3.53G [00:17<00:33, 72.4MB/s]\u001b[A\n",
            " 37%|███▋      | 1.32G/3.53G [00:17<00:26, 91.1MB/s]\u001b[A\n",
            " 38%|███▊      | 1.33G/3.53G [00:17<00:24, 96.9MB/s]\u001b[A\n",
            " 38%|███▊      | 1.34G/3.53G [00:18<00:22, 104MB/s] \u001b[A\n",
            " 38%|███▊      | 1.35G/3.53G [00:18<00:22, 106MB/s]\u001b[A\n",
            " 39%|███▊      | 1.37G/3.53G [00:18<00:21, 107MB/s]\u001b[A\n",
            " 39%|███▉      | 1.38G/3.53G [00:18<00:21, 106MB/s]\u001b[A\n",
            " 39%|███▉      | 1.39G/3.53G [00:18<00:27, 83.9MB/s]\u001b[A\n",
            " 40%|███▉      | 1.40G/3.53G [00:18<00:25, 88.1MB/s]\u001b[A\n",
            " 40%|███▉      | 1.41G/3.53G [00:18<00:23, 98.8MB/s]\u001b[A\n",
            " 40%|████      | 1.42G/3.53G [00:18<00:21, 104MB/s] \u001b[A\n",
            " 41%|████      | 1.44G/3.53G [00:18<00:19, 115MB/s]\u001b[A\n",
            " 41%|████      | 1.45G/3.53G [00:19<00:20, 111MB/s]\u001b[A\n",
            " 41%|████▏     | 1.46G/3.53G [00:19<00:20, 108MB/s]\u001b[A\n",
            " 42%|████▏     | 1.47G/3.53G [00:19<00:19, 115MB/s]\u001b[A\n",
            " 42%|████▏     | 1.48G/3.53G [00:19<00:18, 116MB/s]\u001b[A\n",
            " 42%|████▏     | 1.50G/3.53G [00:19<00:19, 110MB/s]\u001b[A\n",
            " 43%|████▎     | 1.51G/3.53G [00:19<00:23, 94.4MB/s]\u001b[A\n",
            " 43%|████▎     | 1.52G/3.53G [00:19<00:24, 86.7MB/s]\u001b[A\n",
            " 43%|████▎     | 1.52G/3.53G [00:20<00:26, 81.4MB/s]\u001b[A\n",
            " 43%|████▎     | 1.53G/3.53G [00:20<00:25, 82.9MB/s]\u001b[A\n",
            " 44%|████▎     | 1.54G/3.53G [00:20<00:24, 87.0MB/s]\u001b[A\n",
            " 44%|████▍     | 1.55G/3.53G [00:20<00:25, 83.8MB/s]\u001b[A\n",
            " 44%|████▍     | 1.56G/3.53G [00:20<00:26, 79.9MB/s]\u001b[A\n",
            " 44%|████▍     | 1.57G/3.53G [00:20<00:26, 80.6MB/s]\u001b[A\n",
            " 45%|████▍     | 1.58G/3.53G [00:20<00:26, 80.5MB/s]\u001b[A\n",
            " 45%|████▍     | 1.59G/3.53G [00:20<00:24, 85.0MB/s]\u001b[A\n",
            " 45%|████▌     | 1.59G/3.53G [00:20<00:26, 77.1MB/s]\u001b[A\n",
            " 45%|████▌     | 1.61G/3.53G [00:21<00:24, 86.1MB/s]\u001b[A\n",
            " 46%|████▌     | 1.61G/3.53G [00:21<00:25, 79.9MB/s]\u001b[A\n",
            " 46%|████▌     | 1.62G/3.53G [00:21<00:26, 76.1MB/s]\u001b[A\n",
            " 46%|████▌     | 1.63G/3.53G [00:21<00:26, 76.1MB/s]\u001b[A\n",
            " 46%|████▋     | 1.64G/3.53G [00:21<00:29, 69.5MB/s]\u001b[A\n",
            " 47%|████▋     | 1.65G/3.53G [00:21<00:27, 72.6MB/s]\u001b[A\n",
            " 47%|████▋     | 1.65G/3.53G [00:21<00:27, 73.4MB/s]\u001b[A\n",
            " 47%|████▋     | 1.66G/3.53G [00:21<00:24, 80.5MB/s]\u001b[A\n",
            " 47%|████▋     | 1.67G/3.53G [00:22<00:26, 74.8MB/s]\u001b[A\n",
            " 48%|████▊     | 1.68G/3.53G [00:22<00:26, 74.6MB/s]\u001b[A\n",
            " 48%|████▊     | 1.69G/3.53G [00:22<00:27, 72.6MB/s]\u001b[A\n",
            " 48%|████▊     | 1.69G/3.53G [00:22<00:28, 69.2MB/s]\u001b[A\n",
            " 48%|████▊     | 1.70G/3.53G [00:22<00:26, 72.9MB/s]\u001b[A\n",
            " 49%|████▊     | 1.72G/3.53G [00:22<00:21, 89.3MB/s]\u001b[A\n",
            " 49%|████▉     | 1.73G/3.53G [00:22<00:19, 102MB/s] \u001b[A\n",
            " 49%|████▉     | 1.74G/3.53G [00:22<00:18, 105MB/s]\u001b[A\n",
            " 50%|████▉     | 1.75G/3.53G [00:22<00:20, 95.1MB/s]\u001b[A\n",
            " 50%|█████     | 1.77G/3.53G [00:23<00:17, 111MB/s] \u001b[A\n",
            " 50%|█████     | 1.78G/3.53G [00:23<00:16, 117MB/s]\u001b[A\n",
            " 51%|█████     | 1.79G/3.53G [00:23<00:15, 118MB/s]\u001b[A\n",
            " 51%|█████     | 1.80G/3.53G [00:23<00:17, 104MB/s]\u001b[A\n",
            " 51%|█████▏    | 1.82G/3.53G [00:23<00:16, 111MB/s]\u001b[A\n",
            " 52%|█████▏    | 1.83G/3.53G [00:23<00:15, 118MB/s]\u001b[A\n",
            " 52%|█████▏    | 1.84G/3.53G [00:23<00:15, 114MB/s]\u001b[A\n",
            " 52%|█████▏    | 1.85G/3.53G [00:23<00:15, 115MB/s]\u001b[A\n",
            " 53%|█████▎    | 1.87G/3.53G [00:24<00:17, 103MB/s]\u001b[A\n",
            " 53%|█████▎    | 1.88G/3.53G [00:24<00:16, 105MB/s]\u001b[A\n",
            " 53%|█████▎    | 1.89G/3.53G [00:24<00:17, 101MB/s]\u001b[A\n",
            " 54%|█████▎    | 1.90G/3.53G [00:24<00:20, 87.7MB/s]\u001b[A\n",
            " 54%|█████▍    | 1.91G/3.53G [00:24<00:19, 91.2MB/s]\u001b[A\n",
            " 54%|█████▍    | 1.92G/3.53G [00:24<00:18, 96.0MB/s]\u001b[A\n",
            " 55%|█████▍    | 1.93G/3.53G [00:24<00:17, 101MB/s] \u001b[A\n",
            " 55%|█████▍    | 1.94G/3.53G [00:24<00:16, 104MB/s]\u001b[A\n",
            " 55%|█████▌    | 1.95G/3.53G [00:24<00:16, 105MB/s]\u001b[A\n",
            " 56%|█████▌    | 1.96G/3.53G [00:25<00:15, 108MB/s]\u001b[A\n",
            " 56%|█████▌    | 1.97G/3.53G [00:25<00:16, 100MB/s]\u001b[A\n",
            " 56%|█████▌    | 1.98G/3.53G [00:25<00:19, 86.5MB/s]\u001b[A\n",
            " 56%|█████▋    | 1.99G/3.53G [00:25<00:17, 92.3MB/s]\u001b[A\n",
            " 57%|█████▋    | 2.00G/3.53G [00:25<00:17, 95.0MB/s]\u001b[A\n",
            " 57%|█████▋    | 2.01G/3.53G [00:25<00:17, 94.9MB/s]\u001b[A\n",
            " 57%|█████▋    | 2.02G/3.53G [00:25<00:16, 99.3MB/s]\u001b[A\n",
            " 58%|█████▊    | 2.03G/3.53G [00:25<00:15, 102MB/s] \u001b[A\n",
            " 58%|█████▊    | 2.04G/3.53G [00:25<00:15, 102MB/s]\u001b[A\n",
            " 58%|█████▊    | 2.05G/3.53G [00:26<00:17, 91.4MB/s]\u001b[A\n",
            " 58%|█████▊    | 2.06G/3.53G [00:26<00:17, 89.4MB/s]\u001b[A\n",
            " 59%|█████▊    | 2.07G/3.53G [00:26<00:16, 95.2MB/s]\u001b[A\n",
            " 59%|█████▉    | 2.08G/3.53G [00:26<00:18, 85.3MB/s]\u001b[A\n",
            " 59%|█████▉    | 2.09G/3.53G [00:26<00:16, 94.7MB/s]\u001b[A\n",
            " 60%|█████▉    | 2.10G/3.53G [00:26<00:15, 96.3MB/s]\u001b[A\n",
            " 60%|█████▉    | 2.12G/3.53G [00:26<00:15, 99.4MB/s]\u001b[A\n",
            " 60%|██████    | 2.13G/3.53G [00:26<00:14, 101MB/s] \u001b[A\n",
            " 60%|██████    | 2.14G/3.53G [00:27<00:18, 82.6MB/s]\u001b[A\n",
            " 61%|██████    | 2.14G/3.53G [00:27<00:17, 82.9MB/s]\u001b[A\n",
            " 61%|██████    | 2.16G/3.53G [00:27<00:15, 92.5MB/s]\u001b[A\n",
            " 61%|██████▏   | 2.17G/3.53G [00:27<00:15, 97.5MB/s]\u001b[A\n",
            " 62%|██████▏   | 2.18G/3.53G [00:27<00:14, 97.8MB/s]\u001b[A\n",
            " 62%|██████▏   | 2.19G/3.53G [00:27<00:15, 94.6MB/s]\u001b[A\n",
            " 62%|██████▏   | 2.20G/3.53G [00:27<00:14, 98.4MB/s]\u001b[A\n",
            " 63%|██████▎   | 2.21G/3.53G [00:27<00:14, 100MB/s] \u001b[A\n",
            " 63%|██████▎   | 2.22G/3.53G [00:28<00:13, 101MB/s]\u001b[A\n",
            " 63%|██████▎   | 2.23G/3.53G [00:28<00:13, 104MB/s]\u001b[A\n",
            " 63%|██████▎   | 2.24G/3.53G [00:28<00:13, 104MB/s]\u001b[A\n",
            " 64%|██████▎   | 2.25G/3.53G [00:28<00:16, 85.6MB/s]\u001b[A\n",
            " 64%|██████▍   | 2.26G/3.53G [00:28<00:14, 92.2MB/s]\u001b[A\n",
            " 64%|██████▍   | 2.27G/3.53G [00:28<00:13, 96.9MB/s]\u001b[A\n",
            " 65%|██████▍   | 2.28G/3.53G [00:28<00:13, 96.4MB/s]\u001b[A\n",
            " 65%|██████▍   | 2.29G/3.53G [00:28<00:12, 104MB/s] \u001b[A\n",
            " 65%|██████▌   | 2.30G/3.53G [00:28<00:12, 106MB/s]\u001b[A\n",
            " 65%|██████▌   | 2.31G/3.53G [00:29<00:12, 102MB/s]\u001b[A\n",
            " 66%|██████▌   | 2.32G/3.53G [00:29<00:13, 95.8MB/s]\u001b[A\n",
            " 66%|██████▌   | 2.33G/3.53G [00:29<00:12, 100MB/s] \u001b[A\n",
            " 66%|██████▋   | 2.34G/3.53G [00:29<00:13, 98.0MB/s]\u001b[A\n",
            " 67%|██████▋   | 2.35G/3.53G [00:29<00:12, 99.8MB/s]\u001b[A\n",
            " 67%|██████▋   | 2.36G/3.53G [00:29<00:14, 85.0MB/s]\u001b[A\n",
            " 67%|██████▋   | 2.37G/3.53G [00:29<00:13, 89.6MB/s]\u001b[A\n",
            " 67%|██████▋   | 2.38G/3.53G [00:29<00:13, 92.5MB/s]\u001b[A\n",
            " 68%|██████▊   | 2.39G/3.53G [00:29<00:12, 97.3MB/s]\u001b[A\n",
            " 68%|██████▊   | 2.40G/3.53G [00:30<00:12, 97.1MB/s]\u001b[A\n",
            " 68%|██████▊   | 2.41G/3.53G [00:30<00:12, 98.5MB/s]\u001b[A\n",
            " 69%|██████▊   | 2.42G/3.53G [00:30<00:11, 99.4MB/s]\u001b[A\n",
            " 69%|██████▉   | 2.43G/3.53G [00:30<00:12, 91.4MB/s]\u001b[A\n",
            " 69%|██████▉   | 2.44G/3.53G [00:30<00:13, 88.1MB/s]\u001b[A\n",
            " 69%|██████▉   | 2.45G/3.53G [00:30<00:12, 89.7MB/s]\u001b[A\n",
            " 70%|██████▉   | 2.46G/3.53G [00:30<00:13, 82.5MB/s]\u001b[A\n",
            " 70%|██████▉   | 2.47G/3.53G [00:30<00:14, 79.4MB/s]\u001b[A\n",
            " 70%|███████   | 2.47G/3.53G [00:31<00:14, 78.4MB/s]\u001b[A\n",
            " 70%|███████   | 2.48G/3.53G [00:31<00:14, 75.8MB/s]\u001b[A\n",
            " 71%|███████   | 2.49G/3.53G [00:32<00:54, 20.4MB/s]\u001b[A\n",
            " 71%|███████   | 2.50G/3.53G [00:32<00:47, 23.6MB/s]\u001b[A\n",
            " 71%|███████   | 2.50G/3.53G [00:32<00:37, 29.0MB/s]\u001b[A\n",
            " 71%|███████   | 2.51G/3.53G [00:32<00:32, 33.9MB/s]\u001b[A\n",
            " 71%|███████   | 2.52G/3.53G [00:32<00:30, 35.8MB/s]\u001b[A\n",
            " 71%|███████▏  | 2.52G/3.53G [00:32<00:30, 36.1MB/s]\u001b[A\n",
            " 72%|███████▏  | 2.53G/3.53G [00:33<00:28, 38.1MB/s]\u001b[A\n",
            " 72%|███████▏  | 2.53G/3.53G [00:33<00:26, 41.0MB/s]\u001b[A\n",
            " 72%|███████▏  | 2.54G/3.53G [00:33<00:24, 43.2MB/s]\u001b[A\n",
            " 72%|███████▏  | 2.54G/3.53G [00:33<00:19, 54.6MB/s]\u001b[A\n",
            " 72%|███████▏  | 2.56G/3.53G [00:33<00:15, 66.0MB/s]\u001b[A\n",
            " 73%|███████▎  | 2.56G/3.53G [00:33<00:18, 57.3MB/s]\u001b[A\n",
            " 73%|███████▎  | 2.57G/3.53G [00:33<00:16, 61.0MB/s]\u001b[A\n",
            " 73%|███████▎  | 2.58G/3.53G [00:33<00:16, 61.7MB/s]\u001b[A\n",
            " 73%|███████▎  | 2.58G/3.53G [00:34<00:16, 61.7MB/s]\u001b[A\n",
            " 73%|███████▎  | 2.59G/3.53G [00:34<00:15, 65.1MB/s]\u001b[A\n",
            " 74%|███████▎  | 2.60G/3.53G [00:34<00:15, 63.9MB/s]\u001b[A\n",
            " 74%|███████▍  | 2.60G/3.53G [00:34<00:15, 63.7MB/s]\u001b[A\n",
            " 74%|███████▍  | 2.61G/3.53G [00:34<00:14, 68.4MB/s]\u001b[A\n",
            " 74%|███████▍  | 2.62G/3.53G [00:34<00:15, 61.9MB/s]\u001b[A\n",
            " 74%|███████▍  | 2.63G/3.53G [00:34<00:15, 61.4MB/s]\u001b[A\n",
            " 75%|███████▍  | 2.64G/3.53G [00:34<00:13, 71.3MB/s]\u001b[A\n",
            " 75%|███████▍  | 2.64G/3.53G [00:34<00:13, 72.6MB/s]\u001b[A\n",
            " 75%|███████▌  | 2.65G/3.53G [00:35<00:13, 69.0MB/s]\u001b[A\n",
            " 75%|███████▌  | 2.66G/3.53G [00:35<00:15, 62.1MB/s]\u001b[A\n",
            " 75%|███████▌  | 2.67G/3.53G [00:35<00:16, 57.0MB/s]\u001b[A\n",
            " 76%|███████▌  | 2.67G/3.53G [00:35<00:14, 62.3MB/s]\u001b[A\n",
            " 76%|███████▌  | 2.68G/3.53G [00:35<00:14, 63.1MB/s]\u001b[A\n",
            " 76%|███████▌  | 2.69G/3.53G [00:35<00:15, 59.7MB/s]\u001b[A\n",
            " 76%|███████▋  | 2.70G/3.53G [00:35<00:13, 67.8MB/s]\u001b[A\n",
            " 77%|███████▋  | 2.70G/3.53G [00:36<00:12, 68.9MB/s]\u001b[A\n",
            " 77%|███████▋  | 2.71G/3.53G [00:36<00:12, 67.9MB/s]\u001b[A\n",
            " 77%|███████▋  | 2.72G/3.53G [00:36<00:12, 71.1MB/s]\u001b[A\n",
            " 77%|███████▋  | 2.73G/3.53G [00:36<00:12, 71.6MB/s]\u001b[A\n",
            " 78%|███████▊  | 2.74G/3.53G [00:36<00:09, 88.3MB/s]\u001b[A\n",
            " 78%|███████▊  | 2.75G/3.53G [00:36<00:10, 79.2MB/s]\u001b[A\n",
            " 78%|███████▊  | 2.76G/3.53G [00:36<00:08, 95.2MB/s]\u001b[A\n",
            " 79%|███████▊  | 2.77G/3.53G [00:36<00:08, 90.7MB/s]\u001b[A\n",
            " 79%|███████▉  | 2.78G/3.53G [00:36<00:08, 95.3MB/s]\u001b[A\n",
            " 79%|███████▉  | 2.79G/3.53G [00:37<00:09, 79.3MB/s]\u001b[A\n",
            " 79%|███████▉  | 2.80G/3.53G [00:37<00:08, 87.3MB/s]\u001b[A\n",
            " 80%|███████▉  | 2.81G/3.53G [00:37<00:08, 93.6MB/s]\u001b[A\n",
            " 80%|████████  | 2.83G/3.53G [00:37<00:08, 91.4MB/s]\u001b[A\n",
            " 80%|████████  | 2.83G/3.53G [00:37<00:08, 91.1MB/s]\u001b[A\n",
            " 81%|████████  | 2.85G/3.53G [00:37<00:07, 101MB/s] \u001b[A\n",
            " 81%|████████  | 2.86G/3.53G [00:37<00:07, 101MB/s]\u001b[A\n",
            " 81%|████████  | 2.87G/3.53G [00:37<00:07, 101MB/s]\u001b[A\n",
            " 81%|████████▏ | 2.88G/3.53G [00:38<00:08, 82.8MB/s]\u001b[A\n",
            " 82%|████████▏ | 2.89G/3.53G [00:38<00:08, 84.7MB/s]\u001b[A\n",
            " 82%|████████▏ | 2.90G/3.53G [00:38<00:09, 75.6MB/s]\u001b[A\n",
            " 82%|████████▏ | 2.91G/3.53G [00:38<00:07, 84.2MB/s]\u001b[A\n",
            " 83%|████████▎ | 2.92G/3.53G [00:38<00:07, 88.6MB/s]\u001b[A\n",
            " 83%|████████▎ | 2.93G/3.53G [00:38<00:07, 89.1MB/s]\u001b[A\n",
            " 83%|████████▎ | 2.93G/3.53G [00:38<00:07, 89.3MB/s]\u001b[A\n",
            " 83%|████████▎ | 2.95G/3.53G [00:38<00:07, 89.2MB/s]\u001b[A\n",
            " 84%|████████▎ | 2.95G/3.53G [00:39<00:08, 76.3MB/s]\u001b[A\n",
            " 84%|████████▍ | 2.96G/3.53G [00:39<00:07, 85.4MB/s]\u001b[A\n",
            " 84%|████████▍ | 2.97G/3.53G [00:39<00:06, 88.7MB/s]\u001b[A\n",
            " 84%|████████▍ | 2.98G/3.53G [00:39<00:07, 78.9MB/s]\u001b[A\n",
            " 85%|████████▍ | 2.99G/3.53G [00:39<00:06, 84.5MB/s]\u001b[A\n",
            " 85%|████████▌ | 3.00G/3.53G [00:39<00:06, 88.9MB/s]\u001b[A\n",
            " 85%|████████▌ | 3.01G/3.53G [00:39<00:05, 95.1MB/s]\u001b[A\n",
            " 86%|████████▌ | 3.02G/3.53G [00:39<00:05, 99.1MB/s]\u001b[A\n",
            " 86%|████████▌ | 3.03G/3.53G [00:39<00:05, 95.1MB/s]\u001b[A\n",
            " 86%|████████▋ | 3.05G/3.53G [00:40<00:05, 104MB/s] \u001b[A\n",
            " 87%|████████▋ | 3.06G/3.53G [00:40<00:04, 109MB/s]\u001b[A\n",
            " 87%|████████▋ | 3.07G/3.53G [00:40<00:04, 110MB/s]\u001b[A\n",
            " 87%|████████▋ | 3.08G/3.53G [00:40<00:05, 93.3MB/s]\u001b[A\n",
            " 88%|████████▊ | 3.09G/3.53G [00:40<00:05, 93.7MB/s]\u001b[A\n",
            " 88%|████████▊ | 3.10G/3.53G [00:40<00:04, 107MB/s] \u001b[A\n",
            " 88%|████████▊ | 3.11G/3.53G [00:40<00:05, 81.8MB/s]\u001b[A\n",
            " 88%|████████▊ | 3.12G/3.53G [00:41<00:05, 83.4MB/s]\u001b[A\n",
            " 89%|████████▊ | 3.13G/3.53G [00:41<00:05, 77.8MB/s]\u001b[A\n",
            " 89%|████████▉ | 3.14G/3.53G [00:41<00:05, 83.2MB/s]\u001b[A\n",
            " 89%|████████▉ | 3.15G/3.53G [00:41<00:06, 63.7MB/s]\u001b[A\n",
            " 89%|████████▉ | 3.16G/3.53G [00:41<00:07, 55.2MB/s]\u001b[A\n",
            " 90%|████████▉ | 3.17G/3.53G [00:41<00:06, 57.0MB/s]\u001b[A\n",
            " 90%|████████▉ | 3.17G/3.53G [00:41<00:06, 58.5MB/s]\u001b[A\n",
            " 90%|█████████ | 3.18G/3.53G [00:42<00:05, 66.3MB/s]\u001b[A\n",
            " 90%|█████████ | 3.19G/3.53G [00:42<00:04, 75.4MB/s]\u001b[A\n",
            " 91%|█████████ | 3.20G/3.53G [00:42<00:03, 91.0MB/s]\u001b[A\n",
            " 91%|█████████ | 3.21G/3.53G [00:42<00:04, 72.4MB/s]\u001b[A\n",
            " 91%|█████████ | 3.22G/3.53G [00:42<00:04, 74.0MB/s]\u001b[A\n",
            " 91%|█████████▏| 3.23G/3.53G [00:42<00:04, 76.3MB/s]\u001b[A\n",
            " 92%|█████████▏| 3.24G/3.53G [00:42<00:04, 72.7MB/s]\u001b[A\n",
            " 92%|█████████▏| 3.25G/3.53G [00:42<00:04, 74.3MB/s]\u001b[A\n",
            " 92%|█████████▏| 3.25G/3.53G [00:43<00:03, 79.0MB/s]\u001b[A\n",
            " 92%|█████████▏| 3.27G/3.53G [00:43<00:03, 88.4MB/s]\u001b[A\n",
            " 93%|█████████▎| 3.27G/3.53G [00:43<00:03, 73.8MB/s]\u001b[A\n",
            " 93%|█████████▎| 3.29G/3.53G [00:43<00:03, 85.4MB/s]\u001b[A\n",
            " 93%|█████████▎| 3.30G/3.53G [00:43<00:02, 92.1MB/s]\u001b[A\n",
            " 94%|█████████▎| 3.31G/3.53G [00:43<00:02, 96.9MB/s]\u001b[A\n",
            " 94%|█████████▍| 3.32G/3.53G [00:43<00:02, 100MB/s] \u001b[A\n",
            " 94%|█████████▍| 3.33G/3.53G [00:43<00:02, 91.2MB/s]\u001b[A\n",
            " 95%|█████████▍| 3.34G/3.53G [00:43<00:02, 94.1MB/s]\u001b[A\n",
            " 95%|█████████▍| 3.35G/3.53G [00:44<00:02, 93.3MB/s]\u001b[A\n",
            " 95%|█████████▌| 3.36G/3.53G [00:44<00:02, 69.8MB/s]\u001b[A\n",
            " 95%|█████████▌| 3.37G/3.53G [00:44<00:02, 76.9MB/s]\u001b[A\n",
            " 96%|█████████▌| 3.38G/3.53G [00:44<00:01, 84.6MB/s]\u001b[A\n",
            " 96%|█████████▌| 3.39G/3.53G [00:44<00:01, 88.7MB/s]\u001b[A\n",
            " 96%|█████████▋| 3.40G/3.53G [00:44<00:01, 94.9MB/s]\u001b[A\n",
            " 97%|█████████▋| 3.41G/3.53G [00:44<00:01, 98.6MB/s]\u001b[A\n",
            " 97%|█████████▋| 3.42G/3.53G [00:44<00:01, 97.7MB/s]\u001b[A\n",
            " 97%|█████████▋| 3.43G/3.53G [00:45<00:01, 101MB/s] \u001b[A\n",
            " 97%|█████████▋| 3.44G/3.53G [00:45<00:00, 99.8MB/s]\u001b[A\n",
            " 98%|█████████▊| 3.45G/3.53G [00:45<00:00, 94.6MB/s]\u001b[A\n",
            " 98%|█████████▊| 3.46G/3.53G [00:45<00:00, 93.1MB/s]\u001b[A\n",
            " 98%|█████████▊| 3.47G/3.53G [00:45<00:00, 99.4MB/s]\u001b[A\n",
            " 99%|█████████▊| 3.48G/3.53G [00:45<00:00, 105MB/s] \u001b[A\n",
            " 99%|█████████▉| 3.49G/3.53G [00:45<00:00, 71.0MB/s]\u001b[A\n",
            " 99%|█████████▉| 3.51G/3.53G [00:46<00:00, 81.6MB/s]\u001b[A\n",
            "100%|█████████▉| 3.52G/3.53G [00:46<00:00, 91.4MB/s]\u001b[A\n",
            "100%|██████████| 3.53G/3.53G [00:46<00:00, 81.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Daniel Franzen and Jan Disselhoff\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "trusted": true,
        "id": "Gz2yBJTFMBWj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This notebook contains our winning submission to the ARC Prize 2024 Kaggle competition,\n",
        "# scoring 53.5 points on the private evaluation set.\n",
        "# the ARChitects (Daniel Franzen and Jan Disselhoff)"
      ],
      "metadata": {
        "trusted": true,
        "id": "C2sM_EqoMBWk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_runner.py\n",
        "import json\n",
        "import os, sys\n",
        "import bz2\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def indices_required_for_merges(keep_indices, vocab, merges):\n",
        "    merges_lookup = {}\n",
        "    for m in merges:\n",
        "        a, b = m.split(' ') if isinstance(m, str) else m\n",
        "        key = vocab[f'{a}{b}']\n",
        "        if key not in merges_lookup: merges_lookup[key] = set()\n",
        "        merges_lookup[key].add(vocab[a])\n",
        "        merges_lookup[key].add(vocab[b])\n",
        "    to_process = list(keep_indices)\n",
        "    while len(to_process):\n",
        "        for w in merges_lookup.get(to_process.pop(), []):\n",
        "            if w not in keep_indices:\n",
        "                keep_indices[w] = None\n",
        "                to_process.append(w)\n",
        "    return keep_indices\n",
        "\n",
        "def remove_unused_merges(merges, vocab):\n",
        "    return [f'{a} {b}' for a, b in [m.split(' ') if isinstance(m, str) else m for m in merges] if all(w in vocab for w in [a, b, a + b])]\n",
        "\n",
        "def map_special_tokens(data, mapping=None):\n",
        "    tokens = set()\n",
        "    if isinstance(data, dict):\n",
        "        special = data.get('special_tokens')\n",
        "        if special is not None:\n",
        "            for v in special.values():\n",
        "                tokens.update(v['ids'])\n",
        "                if mapping is not None:\n",
        "                    v['ids'] = [mapping.get(i) for i in v['ids'] if i in mapping]\n",
        "    for v in (data.values() if isinstance(data, dict) else data if isinstance(data, list) else []):\n",
        "        tokens.update(map_special_tokens(v, mapping))\n",
        "    return tokens\n",
        "\n",
        "def remove_tokenizer_normalizer(tokenizer):\n",
        "    from tokenizers import Tokenizer\n",
        "    assert tokenizer.is_fast\n",
        "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
        "    if tokenizer_json.get('normalizer') is not None:\n",
        "        tokenizer_json['normalizer'] = None\n",
        "        tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n",
        "\n",
        "def shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order):\n",
        "    from tokenizers import Tokenizer\n",
        "    assert tokenizer.is_fast\n",
        "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
        "    assert tokenizer_json['model']['type'] == \"BPE\"\n",
        "    if keep_special_tokens:\n",
        "        keep_indices.update({k: None for k in tokenizer.all_special_ids})\n",
        "        keep_indices.update({k: None for k in map_special_tokens(tokenizer_json.get('post_processor'))})\n",
        "    keep_indices = indices_required_for_merges(keep_indices, tokenizer_json['model']['vocab'], tokenizer_json['model']['merges'])\n",
        "    if keep_token_order: keep_indices = sorted(keep_indices)\n",
        "    mapping = {old: new for new, old in enumerate(keep_indices)}\n",
        "    tokenizer_json['model']['vocab'] = {k: mapping[v] for k, v in tokenizer_json['model']['vocab'].items() if v in mapping}\n",
        "    tokenizer_json['model']['merges'] = remove_unused_merges(tokenizer_json['model']['merges'], tokenizer_json['model']['vocab'])\n",
        "    special_tokens_order = [t['id'] for t in tokenizer_json['added_tokens']]\n",
        "    assert special_tokens_order==sorted(special_tokens_order)\n",
        "    tokenizer_json['added_tokens'] = sorted([{**t, 'id': mapping[t['id']]} for t in tokenizer_json['added_tokens'] if t['id'] in mapping], key=lambda t: t['id'])\n",
        "    map_special_tokens(tokenizer_json.get('post_processor'), mapping)\n",
        "    tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n",
        "    return mapping, keep_indices\n",
        "\n",
        "def shrink_model_embeddings(model, keep_indices, mapping):\n",
        "    import torch\n",
        "    with torch.no_grad():\n",
        "        row_select = torch.tensor(list(keep_indices))\n",
        "        new_embed_t = torch.index_select(model.get_input_embeddings().weight.data, 0, row_select.to(model.get_input_embeddings().weight.data.device))\n",
        "        new_lm_head = torch.index_select(model.get_output_embeddings().weight.data, 0, row_select.to(model.get_output_embeddings().weight.data.device))\n",
        "        model.resize_token_embeddings(len(keep_indices))\n",
        "        model.get_input_embeddings().weight.data[:] = new_embed_t\n",
        "        model.get_output_embeddings().weight.data[:] = new_lm_head\n",
        "        for config in [model.config, model.generation_config]:\n",
        "            for k, v in list(config.to_dict().items()):\n",
        "                if k.endswith('token_id'):\n",
        "                    setattr(config, k, [mapping.get(t) for t in v] if isinstance(v, list) else mapping.get(v))\n",
        "\n",
        "def shrink_embeddings(model, tokenizer, corpus=None, keep_token_ids=[], keep_tokens=[], remove_token_ids=[], keep_model_tokens=True, keep_special_tokens=True, keep_normalizer=False, keep_token_order=True):\n",
        "    if not keep_normalizer: remove_tokenizer_normalizer(tokenizer)\n",
        "    from collections import OrderedDict  # use as OrderedSet\n",
        "    keep_indices = OrderedDict()\n",
        "    keep_indices.update({k: None for k in keep_token_ids})\n",
        "    keep_indices.update({tokenizer.vocab[t]: None for t in keep_tokens})\n",
        "    if corpus is not None: keep_indices.update({k: None for k in tokenizer(corpus)['input_ids']})\n",
        "    if keep_model_tokens:\n",
        "        for config in [model.config, model.generation_config]:\n",
        "            for k, v in config.to_dict().items():\n",
        "                if k.endswith('token_id'):\n",
        "                    keep_indices.update({k: None for k in (v if isinstance(v, list) else [v])})\n",
        "    keep_indices.pop(None, None)\n",
        "    for idx in remove_token_ids: keep_indices.pop(idx, None)\n",
        "    mapping, keep_indices = shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order)\n",
        "    shrink_model_embeddings(model, keep_indices, mapping=mapping)\n",
        "    return mapping\n",
        "\n",
        "def fix_dtypes(model, fix_weights=True, fix_quant_states=True):\n",
        "    import torch\n",
        "    for module in model.modules():\n",
        "        weight = getattr(module, 'weight', None)\n",
        "        if weight is not None:\n",
        "            if torch.is_floating_point(weight):\n",
        "                if fix_weights and weight.dtype!=model.dtype:\n",
        "                    module.to(model.dtype)\n",
        "            else:\n",
        "                qs = getattr(weight, 'quant_state', None)\n",
        "                if qs is not None:\n",
        "                    if fix_quant_states and qs.dtype!=model.dtype:\n",
        "                        qs.dtype = model.dtype\n",
        "    return model\n",
        "\n",
        "def merge_peft_into_base(model):\n",
        "    print('*** Merge peft model into base model...')\n",
        "    assert is_peft_model(model)\n",
        "    return fix_dtypes(model.merge_and_unload())\n",
        "\n",
        "def save_model(store_path, model=None, tokenizer=None, merge=False):\n",
        "    if merge: model = merge_peft_into_base(model)\n",
        "    if store_path is not None:\n",
        "        assert model is not None or tokenizer is not None\n",
        "        print(f\"*** Saving{' merged' if merge else ''} model/tokenizer to '{store_path}'...\")\n",
        "        if model is not None: model.save_pretrained(store_path)\n",
        "        if tokenizer is not None:\n",
        "            tokenizer.save_pretrained(store_path)\n",
        "            to_delete = os.path.join(store_path, 'tokenizer.model')\n",
        "            if os.path.isfile(to_delete): os.remove(to_delete)\n",
        "    return model\n",
        "\n",
        "def is_unsloth_model(model):\n",
        "    return model.model_tags is not None and 'unsloth' in model.model_tags\n",
        "\n",
        "def is_peft_model(model):\n",
        "    return hasattr(model, 'peft_type')\n",
        "\n",
        "def download_model(repo_id, store_path, get_name=lambda n: os.path.join(n.replace('/', '--'), 'transformers', 'default', '1')):\n",
        "    import os\n",
        "    if os.path.exists(repo_id): return repo_id\n",
        "    model_path = os.path.join(store_path, get_name(repo_id))\n",
        "    if not os.path.exists(model_path):\n",
        "        from huggingface_hub import snapshot_download\n",
        "        download_path = snapshot_download(repo_id=repo_id)\n",
        "        os.makedirs(os.path.split(model_path)[0], exist_ok=True)\n",
        "        os.symlink(download_path, model_path, target_is_directory=True)\n",
        "    return model_path\n",
        "\n",
        "def get_and_fix_peft_weights(store):\n",
        "    print(f\"*** Load peft state_dict from '{store}'...\")\n",
        "    from peft import load_peft_weights\n",
        "    state_dict = load_peft_weights(store)\n",
        "    for k in list(state_dict.keys()):\n",
        "        if 'modules_to_save' in k:\n",
        "            del state_dict[k]\n",
        "            original_module_key = k.replace('.modules_to_save.', '.original_module.')\n",
        "            if original_module_key in state_dict: del state_dict[original_module_key]\n",
        "            assert k.replace('.modules_to_save.', '.') in state_dict\n",
        "    return state_dict\n",
        "\n",
        "def set_peft_weights(model, state_dict):\n",
        "    print(f\"*** Set model state_dict...\")\n",
        "    from peft import set_peft_model_state_dict\n",
        "    res = set_peft_model_state_dict(model, state_dict)\n",
        "    assert not res.unexpected_keys\n",
        "\n",
        "def load_peft_state(model, store):\n",
        "    set_peft_weights(model, get_and_fix_peft_weights(store))\n",
        "\n",
        "def prepare_model(model, mode, tokenizer=None, formatter=None, shrink_embedding=False, dequantize=False, peft=[], local_files_only=False, add_special_tokens={}, set_pad_token=None, keep_tokens=[], keep_normalizer=None, peft_trainable=True, device_map=None, tf_grad_cp=True, tf_use_fa2=True, **kwargs):\n",
        "    if isinstance(model, str):\n",
        "        assert tokenizer is None\n",
        "        print(f\"*** Load base model and tokenizer from '{model}'...\")\n",
        "        if mode=='unsloth_4bit':\n",
        "            assert device_map is None, 'unsupported'\n",
        "            from unsloth import FastLanguageModel\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(model_name=model, dtype=None, load_in_4bit=True, local_files_only=local_files_only, **kwargs)\n",
        "        elif mode in ['transformers', 'transformers_bf16', 'transformers_4bit', 'transformers_bf16_4bit', 'tokenizer_only']:\n",
        "            import torch\n",
        "            model_load_args = {}\n",
        "            if device_map is not None: model_load_args['device_map'] = device_map\n",
        "            if tf_use_fa2: model_load_args['attn_implementation'] = 'flash_attention_2'\n",
        "            if mode in ['transformers_bf16', 'transformers_bf16_4bit']: model_load_args['torch_dtype'] = torch.bfloat16\n",
        "            elif mode in ['transformers_4bit', 'transformers_bf16_4bit']:\n",
        "                from transformers import BitsAndBytesConfig\n",
        "                nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "                model_load_args['quantization_config'] = nf4_config\n",
        "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=local_files_only, **kwargs)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model, **model_load_args) if mode!='tokenizer_only' else None\n",
        "            if tf_grad_cp and model is not None: model.gradient_checkpointing_enable()\n",
        "        else: raise NotImplementedError('Unknown mode.')\n",
        "    if add_special_tokens: tokenizer.add_special_tokens(add_special_tokens)\n",
        "    if set_pad_token is not None: tokenizer.pad_token = set_pad_token\n",
        "    if formatter is not None and not hasattr(formatter, 'corpus'):\n",
        "        formatter = formatter(tokenizer=tokenizer)\n",
        "    if (shrink_embedding<len(tokenizer.vocab) if type(shrink_embedding)==int else shrink_embedding) or keep_normalizer is False:\n",
        "        print('*** Shrink embedding...')\n",
        "        embedding_size_before_shrink = len(tokenizer.vocab)\n",
        "        mapping = shrink_embeddings(model, tokenizer, formatter.get_corpus(), keep_tokens=keep_tokens, keep_normalizer=keep_normalizer)\n",
        "        print(f'*** -> Reduced embedding size from {embedding_size_before_shrink} to {len(mapping)} words.')\n",
        "    if dequantize:\n",
        "        print(f'*** Dequantize model...')\n",
        "        model = model.dequantize()\n",
        "    if len(peft):\n",
        "        peft_trained = True if is_peft_model(model) else None\n",
        "        for i, m in enumerate(peft):\n",
        "            if peft_trained is True: model, peft_trained = merge_peft_into_base(model), None\n",
        "            if isinstance(m, str):\n",
        "                if peft_trained is False:\n",
        "                    _, peft_trained = load_peft_state(model, m), True\n",
        "                else:\n",
        "                    print(f\"*** Load peft model from '{m}'...\")\n",
        "                    # be careful when using unsloth - using PeftModel to load the model will not apply unsloth optimizations\n",
        "                    from peft import PeftModel\n",
        "                    model, peft_trained = PeftModel.from_pretrained(model, m, trainable=peft_trainable), True\n",
        "            else:\n",
        "                assert peft_trained is None\n",
        "                if isinstance(m, dict):\n",
        "                    print('*** Create new peft model...')\n",
        "                    if is_unsloth_model(model):\n",
        "                        from unsloth import FastLanguageModel\n",
        "                        my_get_peft_model = FastLanguageModel.get_peft_model\n",
        "                    else:\n",
        "                        from peft import LoraConfig, get_peft_model\n",
        "                        my_get_peft_model = lambda model, **kwargs: get_peft_model(model, LoraConfig(**kwargs))\n",
        "                    model, peft_trained = my_get_peft_model(model, **m), False\n",
        "                else: assert m is None\n",
        "    return model, tokenizer, formatter\n",
        "\n",
        "def training_run(model, formatter, dataset, train_args, max_seq_length, merge=False, store=None, packing=False, grad_acc_fix=False, optimizers=None):\n",
        "    assert merge is False, \"merge after training does not seen to work (at least with unsloth, saved merged model will cointain the untrained weights!)\"\n",
        "    import torch\n",
        "    from datasets import Dataset\n",
        "    add_train_args = {}\n",
        "    if is_unsloth_model(model):\n",
        "        from unsloth import FastLanguageModel\n",
        "        from unsloth import UnslothTrainer as Trainer\n",
        "        from unsloth import UnslothTrainingArguments as TrainingArguments\n",
        "        from unsloth import is_bfloat16_supported\n",
        "        FastLanguageModel.for_training(model)\n",
        "        add_train_args.update(fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported())\n",
        "    else:\n",
        "        from trl import SFTConfig as TrainingArguments\n",
        "        from trl import SFTTrainer as Trainer\n",
        "        model.train()\n",
        "        add_train_args.update(bf16=True)\n",
        "\n",
        "    formatter.tokenizer.padding_side = 'right'\n",
        "    if is_unsloth_model(model):\n",
        "        for convert_to_float in [model.get_input_embeddings(), model.get_output_embeddings()]:\n",
        "            if convert_to_float.weight.dtype!=torch.float32: convert_to_float.to(torch.float32)\n",
        "\n",
        "    add_args = {}\n",
        "    if optimizers is not None: add_args['optimizers'] = optimizers\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=formatter.tokenizer,\n",
        "        data_collator=formatter.get_data_collator(),\n",
        "        train_dataset=Dataset.from_list(dataset.as_list(formatter)),\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=None,\n",
        "        packing=packing,  # Can make training 5x faster for short sequences.\n",
        "        **add_args,\n",
        "        args=TrainingArguments(\n",
        "            **add_train_args,\n",
        "            **train_args\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print('*** Start training run...')\n",
        "    if grad_acc_fix and is_unsloth_model(model):\n",
        "        from unsloth import unsloth_train\n",
        "        trainer_stats = unsloth_train(trainer)\n",
        "    else:\n",
        "        if is_unsloth_model(model) and train_args['gradient_accumulation_steps']>1: print('*** WARNING: using faulty unsloth gradient accumulation')\n",
        "        trainer_stats = trainer.train()\n",
        "    try: print(f'*** -> Training took {trainer_stats.metrics[\"train_runtime\"]} seconds.')\n",
        "    except: pass\n",
        "    if store is not None: save_model(store, model, formatter.tokenizer, merge=merge)\n",
        "    return model, trainer_stats\n",
        "\n",
        "def inference_load(store, keys=True, result_dict=None, always_read_from_file=False):\n",
        "    if result_dict is None: result_dict = {}\n",
        "    if store is not None:\n",
        "        if keys is True: keys = os.listdir(store)\n",
        "        for key in keys:\n",
        "            if always_read_from_file or key not in result_dict:\n",
        "                try:\n",
        "                    with bz2.BZ2File(os.path.join(store, key)) as f: result_dict[key] = pickle.load(f)\n",
        "                except: continue\n",
        "    return result_dict\n",
        "\n",
        "def inference_save(store, key, outputs):\n",
        "    if store is not None:\n",
        "        os.makedirs(store, exist_ok=True)\n",
        "        with bz2.BZ2File(os.path.join(store, key), 'w') as f: pickle.dump(outputs, f)\n",
        "\n",
        "class Decoder(object):\n",
        "    def __init__(self, formatter, dataset, n_guesses, max_outputs=None, frac_score=False, quiet=False, name='', additional_decoders=None, prob_baseline=None):\n",
        "        self.formatter = formatter\n",
        "        self.dataset = dataset\n",
        "        self.n_guesses = n_guesses\n",
        "        self.decoded_results = {}\n",
        "        self.correct_solutions = {}\n",
        "        self.keys_lim = set()\n",
        "        self.keys_all = set()\n",
        "        self.mult_cnt = {}\n",
        "        self.keys_cnt = {}\n",
        "        self.frac_score = frac_score\n",
        "        self.max_outputs = max_outputs\n",
        "        self.quiet = quiet\n",
        "        self.input_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='input') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n",
        "        self.reply_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='reply') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n",
        "        self.additional_decoders = additional_decoders\n",
        "        self.name = name\n",
        "        self.prob_tracker = {}\n",
        "        self.prob_tracker_best = {}\n",
        "        self.prob_baseline = prob_baseline\n",
        "\n",
        "    def score(self, *to_score):\n",
        "        scores = [(sum(1/self.mult_cnt[k.split('_')[0]] for k in s) if self.frac_score else len(s)) for s in to_score]\n",
        "        score_cnt = len(self.mult_cnt if self.frac_score else self.keys_cnt)\n",
        "        return scores, score_cnt\n",
        "\n",
        "    def from_store(self, store, **kwargs):\n",
        "        for key, outputs in inference_load(store).items():\n",
        "            self.process(key, outputs, **kwargs)\n",
        "        return self\n",
        "\n",
        "    def score_fmt(self, v):\n",
        "        return f'{v:5.1f}' if self.frac_score else f'{v:3}'\n",
        "\n",
        "    def process_single_output(self, key, output_len, decoded, print_func=print, len_info=None, device_info=None):\n",
        "        import numpy as np\n",
        "        inv_mod = {k: v if k.endswith('val') else self.dataset.invert_mod(v, key, inv_perm=(k.startswith('output') or k.startswith('score_all'))) for k, v in decoded.items()}\n",
        "        base_key = key.split('.')[0]\n",
        "        self.decoded_results[base_key] = self.decoded_results.get(base_key, {})\n",
        "        self.decoded_results[base_key][key] = inv_mod\n",
        "        output = inv_mod.get('output')\n",
        "        score = inv_mod.get('score')\n",
        "\n",
        "        # quick scoring\n",
        "        self.keys_cnt[base_key] = self.keys_cnt.get(base_key, 0) + 1\n",
        "        mult_key, mult_sub = (base_key.split('_') + ['0'])[:2]\n",
        "        self.mult_cnt[mult_key] = max(self.mult_cnt.get(mult_key, 0), int(mult_sub) + 1)\n",
        "        if len(self.dataset.replies):\n",
        "            correct_solution = self.dataset.replies.get(base_key)\n",
        "            if correct_solution is not None:\n",
        "                correct_solution = correct_solution[0]\n",
        "                self.correct_solutions[base_key] = correct_solution\n",
        "                is_correct = correct_solution is not None and np.array_equal(correct_solution, output)\n",
        "                if is_correct:\n",
        "                    self.keys_all.add(base_key)\n",
        "                    if self.keys_cnt[base_key] <= self.n_guesses: self.keys_lim.add(base_key)\n",
        "            corr_str = 'cant_decode' if output is None else 'sol_unknown' if correct_solution is None else 'ALL_CORRECT' if is_correct else 'bad_xy_size' if np.shape(correct_solution)!=np.shape(output) else 'bad_content'\n",
        "            (score_lim, score_all), score_cnt = self.score(self.keys_lim, self.keys_all)\n",
        "\n",
        "            tp_arr = (key.count('transpose') + key.count('rot90')) % 2\n",
        "            msc = None if score is None else np.sum(score)\n",
        "            fsc = inv_mod.get('score_val')\n",
        "            if output is not None and fsc is not None:\n",
        "                pt = self.prob_tracker[base_key] = self.prob_tracker.get(base_key, {})\n",
        "                hash = tuple(map(tuple, output))\n",
        "                prob = pt[hash] = pt.get(hash, 0) + (np.exp(fsc) if self.prob_baseline is None else fsc - np.log(self.prob_baseline))\n",
        "                current_best = self.prob_tracker_best.get(base_key)\n",
        "                if current_best is None or current_best[0]<prob:\n",
        "                    self.prob_tracker_best[base_key] = (prob, output)\n",
        "            fmt_name = f'{self.name}: ' if self.name else ''\n",
        "            msc_print = f'{min(-msc, 9.99999):7.5f}' if msc is not None else 'unknown'\n",
        "            fsc_print = f'{min(-fsc, 9.99999):7.5f}' if fsc is not None else 'unknown'\n",
        "            if not self.quiet: print_func(f\" {fmt_name}acc: {self.score_fmt(score_lim)}/{score_cnt:3}={min(score_lim/score_cnt, 0.999):5.1%} (2-guess), {self.score_fmt(score_all)}/{score_cnt:3}={min(score_all/score_cnt, 0.999):5.1%} (any);{f' {device_info}' if device_info else ''} tok:{self.input_len[tp_arr].get(base_key, '?'):>4}+{self.reply_len[tp_arr].get(base_key, '?'):>3}>{'n/a' if output_len is None else output_len:>3} {corr_str}:{msc_print}|{fsc_print} [{key}]\")\n",
        "\n",
        "    def get_current_best(self, base_key):\n",
        "        current_best = self.prob_tracker_best.get(base_key)\n",
        "        return None if current_best is None else current_best[1]\n",
        "\n",
        "    def process_single_decode(self, key, de_tokenized, print_func=print, **kwargs):\n",
        "        if len(de_tokenized)==3 and not isinstance(de_tokenized[1], float):  # for backwards compatibility\n",
        "            output_len, *data = de_tokenized\n",
        "            score_val = None\n",
        "        else: output_len, score_val, *data = de_tokenized\n",
        "        if self.formatter is None:\n",
        "            assert len(data) == 1\n",
        "            decoded = [data[0]]\n",
        "        else: decoded = self.formatter.decode_to_array(*data)\n",
        "        #if len(decoded)==2:\n",
        "        #    same = np.array_equal(decoded[0].get('output'), decoded[1].get('output'))\n",
        "        #    print_func(f\"is_identical: {same}\")\n",
        "        #    if not same: for i in range(2): print_func(str(decoded[i].get('output')))\n",
        "        for d in decoded: d['score_val'] = score_val\n",
        "        for i, dec in enumerate(decoded):\n",
        "            if i==0: self.process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n",
        "            elif self.additional_decoders:\n",
        "                if i-1<len(self.additional_decoders): self.additional_decoders[i-1].process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n",
        "                else: print_func(f'{key} no decoder available for output #{i}')\n",
        "            else: self.process_single_output(f'{key}.fix{i}', output_len, dec, print_func=print_func, **kwargs)\n",
        "\n",
        "    def process(self, key, de_tokenized, **kwargs):\n",
        "        for i, d in enumerate(de_tokenized):\n",
        "            if self.max_outputs is None or i<=self.max_outputs:\n",
        "                self.process_single_decode(f'{key}.out{i}', d, **kwargs)\n",
        "\n",
        "    def get_unsolved_keys(self):\n",
        "        unsolved = []\n",
        "        for base_key, reply in self.dataset.replies.items():\n",
        "            if not any(np.array_equal(reply[0], s.get('output')) for s in self.decoded_results.get(base_key, {}).values()):\n",
        "                unsolved.append(base_key)\n",
        "        return unsolved\n",
        "\n",
        "    def run_selection_algo(self, selection_algorithm):\n",
        "        return {bk: (selection_algorithm({k: g for k, g in v.items() if g.get('output') is not None}) if any(g.get('output') is not None for g in v.values()) else []) for bk, v in self.decoded_results.items()}\n",
        "\n",
        "    def benchmark_selection_algos(self, selection_algorithms, skip_failed=True):\n",
        "        import numpy as np\n",
        "        results = {}\n",
        "        print('*** Benchmark selection algorithms...')\n",
        "        for selection_algorithm in selection_algorithms:\n",
        "            name = selection_algorithm.__name__\n",
        "            try:\n",
        "                selected = self.run_selection_algo(selection_algorithm)\n",
        "                if self.formatter is not None:\n",
        "                    for sols in selected.values():\n",
        "                        for s in sols:\n",
        "                            assert self.formatter.is_valid_solution(s), f'found invalid solutions {s}'\n",
        "                correct_keys = {k for k, v in selected.items() if self.correct_solutions.get(k) is not None and any(np.array_equal(guess, self.correct_solutions[k]) for guess in v[:self.n_guesses])}\n",
        "                (score,), score_cnt = self.score(correct_keys)\n",
        "                results[name] = score\n",
        "                print(f\" acc: {score:5.1f}/{score_cnt:3}={score/score_cnt:6.2%} ('{name}')\")\n",
        "            except:\n",
        "                print(f\" {'execution failed':>21} ('{name}')\")\n",
        "                if not skip_failed: raise\n",
        "        return results\n",
        "\n",
        "    def calc_augmented_scores(self, model, base_keys=None, store=None, seed=0, max_len=None, make_unique=True, quiet=False, **kwargs):\n",
        "        if base_keys is None: base_keys = list(self.decoded_results.keys())\n",
        "        if store is not None: store = f'{store}_new'  # new format is not backwards compatible, so use new folder\n",
        "        for bk in (base_keys if quiet else tqdm(base_keys, desc='calculate augmented scores', file=sys.stdout)):\n",
        "            res = self.decoded_results.get(bk, {})\n",
        "            known_scores = {}\n",
        "            for k, v in sorted(res.items()):\n",
        "                if 'output' in v:\n",
        "                    k_store = None if store is None else os.path.join(store, k)\n",
        "                    id = tuple(map(tuple, v['output']))\n",
        "                    if not (make_unique and id in known_scores):\n",
        "                        try:\n",
        "                            assert k_store is not None\n",
        "                            with bz2.BZ2File(k_store) as f: known_scores[id] = pickle.load(f)\n",
        "                            if isinstance(known_scores[id], list): known_scores[id] = dict(score_multi=known_scores[id])  # for backwards compatibility\n",
        "                            k_store = None\n",
        "                        except:\n",
        "                            temp_dataset = self.dataset.__class__(\n",
        "                                keys=[bk],\n",
        "                                queries={bk: self.dataset.queries.get(bk)},\n",
        "                                replies={bk: [v['output'].tolist()]},\n",
        "                            )\n",
        "                            temp_decoder = self.__class__(self.formatter, temp_dataset, n_guesses=self.n_guesses, quiet=True)\n",
        "                            temp_dataset = temp_dataset.augment(**kwargs, seed=(seed+hash(k)+hash(id)) % 1024**2, quiet=True)\n",
        "                            if max_len is not None: temp_dataset = temp_dataset.cut_to_len(formatter=self.formatter, name='input', max_len=max_len, quiet=True)\n",
        "                            for x in temp_dataset.as_list(self.formatter): calc_score(**x, formatter=self.formatter, model=model, decoder=temp_decoder)\n",
        "                            known_scores[id] = dict(\n",
        "                                score_multi=[np.sum(x['score']) for x in temp_decoder.decoded_results[bk].values()],\n",
        "                                score_multi_nl=[x['score_val'] for x in temp_decoder.decoded_results[bk].values()],\n",
        "                                score_multi_array=np.array([x['score'] for x in temp_decoder.decoded_results[bk].values()]),\n",
        "                                score_multi_array_cum=np.array([x['score_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n",
        "                                score_multi_array_all=np.array([x['score_all'] for x in temp_decoder.decoded_results[bk].values()]),\n",
        "                                score_multi_array_all_cum=np.array([x['score_all_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n",
        "                            )\n",
        "                            if k_store is not None:\n",
        "                                os.makedirs(store, exist_ok=True)\n",
        "                                with bz2.BZ2File(k_store, 'w') as f: pickle.dump(known_scores[id], f)\n",
        "                    v.update(known_scores[id])\n",
        "\n",
        "def turbo_dfs(model, logits, path, eos_token_id, max_new_tokens, max_score, max_score_greedy, temperature, suppress_tokens, torch, score=0.0, pos=0, cache=None):\n",
        "    logits, next_logits = logits[0], (logits[1:] if len(logits)>1 else None)\n",
        "    nll = -(logits / temperature).detach().float().log_softmax(-1).cpu().numpy()\n",
        "    greedy_index = nll.argmin(-1).item()\n",
        "    nll = list(enumerate(nll))\n",
        "    if path: nll[0], nll[path[0]], path = nll[path[0]], nll[0], path[1:]  # follow precomputed path first\n",
        "    suffixes = []\n",
        "    for i, s in nll:\n",
        "        next_score = score + s\n",
        "        allowed_max_score = max_score_greedy if i==greedy_index else max_score\n",
        "        if next_score < allowed_max_score:\n",
        "            if i==eos_token_id: next_suffixes = [(next_score, [], [])]\n",
        "            elif max_new_tokens>1:\n",
        "                if next_logits is None:\n",
        "                    if pos<cache[0][0][0].shape[2]: cache[0] = tuple(tuple(c[:, :, :pos] for c in l) for l in cache[0])\n",
        "                    next_logits, cache[0] = model(\n",
        "                        input_ids= torch.full((1,1), i, device=model.device),\n",
        "                        position_ids=torch.full((1,1), pos, device=model.device),\n",
        "                        past_key_values=cache[0],\n",
        "                    )[:2]\n",
        "                    next_logits = next_logits[0]  # unbatch\n",
        "                next_suffixes = turbo_dfs(model, logits=next_logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens-1, max_score=max_score, max_score_greedy=allowed_max_score, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=next_score, pos=pos+1, cache=cache)\n",
        "            else: next_suffixes = []\n",
        "            for suffix in next_suffixes:\n",
        "                suffix[1].append(i)\n",
        "                suffix[2].append(logits)\n",
        "            suffixes.extend(next_suffixes)\n",
        "        next_logits = None\n",
        "    return suffixes\n",
        "\n",
        "def inference_turbo_dfs(model, input_ids, eos_token_id, max_new_tokens, min_prob, min_prob_greedy=1, temperature=0.9, suppress_tokens=[], path=[], attention_mask=None):\n",
        "    import torch\n",
        "    with torch.no_grad():\n",
        "        assert attention_mask is None or attention_mask.all(), 'not implemented'\n",
        "        input_ids = torch.as_tensor(input_ids, device=model.device, dtype=int)\n",
        "        if input_ids.ndim==2: input_ids = input_ids.squeeze(0)\n",
        "        assert input_ids.ndim==1, 'batching not supported'\n",
        "        max_score = -np.log(min_prob)\n",
        "        max_score_greedy = (-np.log(min_prob_greedy)) if min_prob_greedy>0 else float('inf')  # avoid throwing numpy error\n",
        "        max_score_greedy = max(max_score, max_score_greedy)\n",
        "        if path is None: path = []\n",
        "        if len(path) and path[-1]==eos_token_id: path = path[:-1]\n",
        "        with torch.no_grad():\n",
        "            full_path = input_ids\n",
        "            if len(path): full_path = torch.cat([full_path, torch.as_tensor(path, device=model.device)])\n",
        "            logits, cache = model(input_ids=full_path[np.newaxis])[:2]\n",
        "            logits = logits[0, len(input_ids)-1:]\n",
        "        result = turbo_dfs(model, logits=logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens, max_score=max_score, max_score_greedy=max_score_greedy, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=0.0, pos=len(input_ids), cache=[cache])\n",
        "        return sorted([(score_val, np.array(suffix[::-1]), torch.stack(score_arr[::-1]).float().cpu().numpy()) for score_val, suffix, score_arr in result], key=lambda x:x[0])\n",
        "\n",
        "def inference_step(tokenized, model, remove_token_type_ids=True, num_beams=1, formatter=None, min_prob=None, current_best=None, **kwargs):\n",
        "    import torch\n",
        "    if remove_token_type_ids: tokenized.pop('token_type_ids', None)\n",
        "    if min_prob is not None:\n",
        "        assert num_beams==1\n",
        "        gen = inference_turbo_dfs(model, **tokenized.to(model.device), path=current_best, min_prob=min_prob, eos_token_id=formatter.tokenizer.eos_token_id, **kwargs)\n",
        "        tokens_out = [[g[1] for g in gen]]\n",
        "        scores_out = [[g[2] for g in gen]]\n",
        "    elif is_unsloth_model(model) and num_beams > 1:\n",
        "        assert False, 'unsloth does not support beam search'\n",
        "    else:\n",
        "        gen = model.generate(**tokenized.to(model.device), return_dict_in_generate=True, output_logits=True, use_cache=True, **kwargs)\n",
        "        tokens_out = gen['sequences'][:, torch.newaxis, tokenized['input_ids'].shape[-1]:].cpu().numpy().copy()\n",
        "        scores_out = torch.stack(gen['logits'], axis=-2)[:, torch.newaxis].float().cpu().numpy().copy()\n",
        "    return tokens_out, scores_out\n",
        "\n",
        "def process_inference_output(key, outputs, formatter, store=None, decoder=None, decoder_args={}):\n",
        "    de_tokenized = [formatter.de_tokenize(*output) for output in zip(*outputs)]\n",
        "    inference_save(store, key, de_tokenized)\n",
        "    if decoder is not None: decoder.process(key, de_tokenized, **decoder_args)\n",
        "    return de_tokenized\n",
        "\n",
        "def inference_run_v2(model, formatter, dataset, decoder=None, max_new_tokens=None, max_batch_size=1, store=None, result_dict=None, rerun_empty=False, retrain=None, use_turbo=False, group_multi_output=True, **kwargs):\n",
        "    import torch\n",
        "    assert max_batch_size==1, 'unsupported'\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print('*** Load stored data...')\n",
        "        if result_dict is None: result_dict = {}\n",
        "        result_dict = inference_load(store, dataset.keys, result_dict)\n",
        "        by_base_key = {}\n",
        "        needs_rerun = {}\n",
        "        base_key_list = []\n",
        "        for key in dataset.keys:\n",
        "            base_key = key.split('.')[0]\n",
        "            if group_multi_output: base_key = base_key.split('_')[0]\n",
        "            if base_key not in by_base_key: base_key_list.append(base_key)\n",
        "            bk_list = by_base_key[base_key] = by_base_key.get(base_key, [])\n",
        "            bk_list.append(key)\n",
        "        for base_key, keys in by_base_key.items():\n",
        "            for key in keys:\n",
        "                de_tokenized = result_dict.get(key)\n",
        "                if de_tokenized is None or (rerun_empty and not de_tokenized):\n",
        "                    bk_list = needs_rerun[base_key] = needs_rerun.get(base_key, [])\n",
        "                    bk_list.append(key)\n",
        "                elif decoder is not None: decoder.process(key, de_tokenized)\n",
        "\n",
        "        formatter.tokenizer.padding_side = 'left'\n",
        "        if max_new_tokens is None: max_new_tokens = formatter.max_new_tokens()\n",
        "        if is_unsloth_model(model):\n",
        "            from unsloth import FastLanguageModel\n",
        "            FastLanguageModel.for_inference(model)\n",
        "        else: model.eval()\n",
        "\n",
        "        print('*** Start inference run...')\n",
        "    try:\n",
        "        with tqdm(base_key_list, file=sys.stdout) as pbar:\n",
        "            for base_key in pbar:\n",
        "                run_keys = needs_rerun.get(base_key)\n",
        "                if run_keys:\n",
        "                    if retrain is not None:\n",
        "                        retrain_dataset = dataset.keep_key_startswith(base_key)\n",
        "                        print(f\"retraining model for key '{base_key}' (retrain_dataset_size={len(retrain_dataset.keys)})\")\n",
        "                        retrain(model, retrain_dataset)\n",
        "                        if is_unsloth_model(model): FastLanguageModel.for_inference(model)\n",
        "                    with torch.no_grad():\n",
        "                        for key in run_keys:\n",
        "                            input_text = dataset.get(key, formatter)['input']\n",
        "                            batch = formatter.tokenizer([input_text], return_tensors='pt')\n",
        "                            current_best = decoder.get_current_best(key.split('.')[0]) if use_turbo else None\n",
        "                            if current_best is not None:\n",
        "                                current_best = dataset.forward_mod(current_best, key)\n",
        "                                current_best = formatter.fmt_reply([current_best])\n",
        "                                current_best = formatter.tokenizer(input_text+current_best)['input_ids'][batch['input_ids'].shape[-1]:]\n",
        "                            batch_out = inference_step(batch, model, formatter=formatter, max_new_tokens=max_new_tokens, current_best=current_best, **kwargs)\n",
        "                            outputs = [x[0] for x in batch_out]\n",
        "                            result_dict[key] = process_inference_output(key, outputs, formatter, store=store, decoder=decoder, decoder_args=dict(print_func=pbar.write))\n",
        "        print('*** Completed inference run.')\n",
        "    except KeyboardInterrupt: print('*** Ctrl+C pressed, stopping inference run.')\n",
        "    return result_dict\n",
        "\n",
        "class Retrainer(object):\n",
        "    def __init__(self, n, aug_opts, reload_state_dict=None, **kwargs):\n",
        "        self.n = n\n",
        "        self.aug_opts = aug_opts\n",
        "        self.reload_state_dict = reload_state_dict\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def preprocess(self, dataset):\n",
        "        ds = [dataset.augment(quiet=True, shfl_keys=True, **self.aug_opts) for _ in range((self.n-1)//dataset.length()+1)]\n",
        "        ds = ds[0] if len(ds)==1 else ds[0].append(*ds[1:])\n",
        "        ds, _ = ds.split_at_pos(self.n)\n",
        "        return ds\n",
        "\n",
        "    def __call__(self, model, dataset):\n",
        "        if self.reload_state_dict is not None: set_peft_weights(model, self.reload_state_dict)\n",
        "        assert is_unsloth_model(model), 'not implemented'\n",
        "        if is_unsloth_model(model):\n",
        "            from unsloth import FastLanguageModel\n",
        "            FastLanguageModel.for_training(model)\n",
        "        else: model.train()\n",
        "        training_run(model, dataset=self.preprocess(dataset), **self.kwargs)\n",
        "\n",
        "def calc_score(key, input, reply, formatter, model, store=None, decoder=None, **_):\n",
        "    import torch\n",
        "    with torch.no_grad():\n",
        "        input_len = len(formatter.tokenizer(input)['input_ids'])\n",
        "        tokenized = formatter.tokenizer([input+reply], return_tensors='pt')\n",
        "        reply_tok = tokenized['input_ids'][0][input_len:].cpu().numpy().copy()\n",
        "        reply_log = model.forward(**tokenized.to(model.device))['logits'][0, input_len-1: -1].float().cpu().numpy().copy()\n",
        "        process_inference_output(key, (reply_tok[torch.newaxis], reply_log[torch.newaxis]), formatter, store=store, decoder=decoder)\n",
        "\n",
        "def mem_info(gpu_id=0):\n",
        "    import torch\n",
        "    try:\n",
        "        gpu_stats = torch.cuda.get_device_properties(gpu_id)\n",
        "        usage = torch.cuda.max_memory_reserved() / 1024**3\n",
        "        avail = gpu_stats.total_memory / 1024**3\n",
        "        print(f\"*** GPU: {gpu_stats.name}, used {usage:.3} / {avail:.3} GB.\")\n",
        "    except: print('*** Exception occured when getting memory stats.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.690367Z",
          "iopub.execute_input": "2024-11-07T17:26:31.691049Z",
          "iopub.status.idle": "2024-11-07T17:26:31.720311Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.691014Z",
          "shell.execute_reply": "2024-11-07T17:26:31.7195Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9HjpnVYMBWk",
        "outputId": "c50d8124-bb18-4cff-a72e-537f20e3393b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_runner.py\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile arc_loader.py\n",
        "import json\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import os, sys\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "def cut_at_token(output, token_id):\n",
        "    eos_positions = (output==token_id).nonzero()[0]\n",
        "    return output[:eos_positions[0]] if len(eos_positions) else output\n",
        "\n",
        "def shuffled(data_list):\n",
        "    return np.random.permutation(data_list).tolist()\n",
        "\n",
        "def permute_mod(a, descriptor, invert=False):\n",
        "    permutation = [int(i) for i in descriptor if str(i).isdigit()]\n",
        "    assert sorted(permutation)==list(range(10))\n",
        "    a = np.asarray(a)\n",
        "    if a.ndim==3:\n",
        "        if not invert: permutation = np.argsort(permutation)\n",
        "        a = a[..., permutation]\n",
        "    else:\n",
        "        assert a.ndim==2\n",
        "        if invert: permutation = np.argsort(permutation)\n",
        "        a = np.asarray(permutation)[a]\n",
        "    return a\n",
        "\n",
        "def permute_rnd_col_(query):\n",
        "    permutation = [0]+(1+np.random.permutation(9)).tolist()\n",
        "    return 'permute' + ''.join(map(str, permutation))\n",
        "\n",
        "def permute_rnd_all_(query):\n",
        "    permutation = np.random.permutation(10).tolist()\n",
        "    return 'permute' + ''.join(map(str, permutation))\n",
        "\n",
        "def permute_cnt_col_(query):\n",
        "    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n",
        "    permutation = [0]+sorted(np.random.permutation(9)+1, key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n",
        "    return 'permute' + ''.join(map(str, permutation))\n",
        "\n",
        "def permute_cnt_all_(query):\n",
        "    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n",
        "    permutation = sorted(np.random.permutation(10), key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n",
        "    return 'permute' + ''.join(map(str, permutation))\n",
        "\n",
        "permute_rnd_col = (permute_mod, permute_rnd_col_)\n",
        "permute_rnd_all = (permute_mod, permute_rnd_all_)\n",
        "permute_cnt_col = (permute_mod, permute_cnt_col_)\n",
        "permute_cnt_all = (permute_mod, permute_cnt_all_)\n",
        "permute_None = (np.copy, None)\n",
        "\n",
        "class ArcDataset(object):\n",
        "    @staticmethod\n",
        "    def forward_mod(a, key, use_perm=True, is_output=True):\n",
        "        if a is None: return a\n",
        "        for op in key.split('.')[1:]:\n",
        "            if op.startswith('I'):\n",
        "                if is_output: continue\n",
        "                op = op[1:]\n",
        "            if   op=='rot90':              a = np.rot90(a)\n",
        "            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n",
        "            elif op.startswith('permute'): a = permute_mod(a, op, invert=False) if use_perm else a\n",
        "            elif op.startswith('copy'):    a = np.copy(a)\n",
        "            elif op.startswith('out'):     a = a\n",
        "            elif op.startswith('ex'):      a = a\n",
        "            elif op.startswith('fix'):     a = a\n",
        "            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n",
        "            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n",
        "        return a\n",
        "\n",
        "    @staticmethod\n",
        "    def invert_mod(a, key, inv_perm=True, is_output=True):\n",
        "        if a is None: return a\n",
        "        for op in key.split('.')[1:][::-1]:\n",
        "            if op.startswith('I'):\n",
        "                if is_output: continue\n",
        "                op = op[1:]\n",
        "            if   op=='rot90':              a = np.rot90(np.rot90(np.rot90(a)))\n",
        "            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n",
        "            elif op.startswith('permute'): a = permute_mod(a, op, invert=True) if inv_perm else a\n",
        "            elif op.startswith('copy'):    a = np.copy(a)\n",
        "            elif op.startswith('out'):     a = a\n",
        "            elif op.startswith('ex'):      a = a\n",
        "            elif op.startswith('fix'):     a = a\n",
        "            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n",
        "            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n",
        "        return a\n",
        "\n",
        "    def __init__(self, queries, replies={}, keys=None, is_orig=False, is_fake=False):\n",
        "        if keys is not None: keys = [k for k in keys if k is not None]\n",
        "        self.queries = queries if keys is None else {k: queries[k] for k in keys}\n",
        "        self.replies = replies if keys is None else {k: replies[k] for k in keys if k in replies}\n",
        "        self.is_orig = is_orig\n",
        "        self.is_fake = is_fake\n",
        "        self.keys = sorted(queries.keys()) if keys is None else keys\n",
        "        self.faulty = {}\n",
        "        self.transposed_dataset = None\n",
        "\n",
        "    @classmethod\n",
        "    def empty(cls):\n",
        "        return cls(queries={}, replies={}, keys=[])\n",
        "\n",
        "    def change_keys(self, keys, keep_flags=False):\n",
        "        flags = dict(is_fake=self.is_fake, is_orig=self.is_orig) if keep_flags else {}\n",
        "        return self.__class__(queries=self.queries, replies=self.replies, keys=keys, **flags)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, queries_file):\n",
        "        print(f\"*** Load challanges from '{queries_file}'...\")\n",
        "        with open(queries_file) as f: queries = f.read()\n",
        "        import os\n",
        "        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'): #Real submit\n",
        "            is_fake = False\n",
        "        else: #Fake run\n",
        "            is_fake = True\n",
        "        #is_fake = hashlib.md5(queries.encode('utf-8')).hexdigest().lower()=='a6b7dac3cab03abf2eb333e16610d6dc'\n",
        "        if is_fake: print(\"*** -> Fake test set detected, setting flag 'is_fake' to True.\")\n",
        "        return cls(\n",
        "            queries=json.loads(queries),\n",
        "            is_fake=is_fake,\n",
        "            is_orig=True,\n",
        "        )\n",
        "\n",
        "    def load_replies(self, replies_file):\n",
        "        print(f\"*** Load solutions from '{replies_file}'...\")\n",
        "        with open(replies_file) as f: replies = f.read()\n",
        "        replies_parsed = json.loads(replies)\n",
        "        self.replies = {k: replies_parsed[k] for k in self.keys}\n",
        "        return self\n",
        "\n",
        "    def split_multi_replies(self):\n",
        "        key_indices = [(k, i) for k in self.keys for i in range(len(self.queries[k]['test']))]\n",
        "        return self.__class__(\n",
        "            keys=[f'{k}_{i}' for k, i in key_indices],\n",
        "            queries={f'{k}_{i}': {'train': self.queries[k]['train'], 'test': [self.queries[k]['test'][i]]} for k, i in key_indices},\n",
        "            replies={f'{k}_{i}': [self.replies[k][i]] for k, i in key_indices if k in self.replies},\n",
        "        )\n",
        "\n",
        "    def move_test_to_train(self):\n",
        "        new_queries = {k: {'train': self.queries[k]['train'] + [{**t, 'output': self.replies[k][i]} for i, t in enumerate(self.queries[k]['test'])], 'test': []} for k in self.keys}\n",
        "        return self.__class__(queries=new_queries, keys=[k for k in self.keys])\n",
        "\n",
        "    def last_train_ex_for_test(self):\n",
        "        assert not self.replies\n",
        "        new_queries = {k: {'train': self.queries[k]['train'][:-1], 'test': [{'input': self.queries[k]['train'][-1]['input']}]} for k in self.keys}\n",
        "        new_replies = {k: [self.queries[k]['train'][-1]['output']] for k in self.keys}\n",
        "        return self.__class__(queries=new_queries, replies=new_replies, keys=[k for k in self.keys])\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def shuffled(self, seed=None):\n",
        "        if seed is not None: np.random.seed(seed)\n",
        "        return self.__class__(queries=self.queries, replies=self.replies, keys=shuffled(self.keys))\n",
        "\n",
        "    def sorted(self, **kwargs):\n",
        "        return self.__class__(queries=self.queries, replies=self.replies, keys=sorted(self.keys, **kwargs))\n",
        "\n",
        "    def append(*datasets):\n",
        "        return datasets[0].__class__(\n",
        "            queries={k: v for d in datasets for k, v in d.queries.items()},\n",
        "            replies={k: v for d in datasets for k, v in d.replies.items()},\n",
        "            keys   =[k    for d in datasets for k    in d.keys           ],\n",
        "        )\n",
        "\n",
        "    def sort_ex_by_input_size(self, seed=42, reverse=False):\n",
        "        np.random.seed(seed)\n",
        "        sort_key = lambda ex: np.prod(np.shape(ex['input']))\n",
        "        new_queries = {k2: {k: (sorted(np.random.permutation(np.array(v, dtype=object)), key=sort_key, reverse=reverse) if k=='train' else v) for k, v in v2.items()} for k2, v2 in self.queries.items()}\n",
        "        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n",
        "\n",
        "    def interleave(self, block_size, num_gpus=None):\n",
        "        keys = np.reshape(self.keys, (-1, block_size)).T\n",
        "        if num_gpus is None: return self.change_keys(keys.ravel().tolist())\n",
        "        ret, num_gpus = (None, num_gpus) if isinstance(num_gpus, int) else num_gpus\n",
        "        keys = np.concatenate([keys, np.full((-keys.shape[0]%num_gpus, keys.shape[1]), None)])\n",
        "        keys = np.reshape(keys, (keys.shape[0]//num_gpus, num_gpus, -1)).swapaxes(0, 1).reshape(num_gpus, -1)\n",
        "        new_datasets = [self.change_keys(gpu_keys.tolist()) for gpu_keys in keys]\n",
        "        return new_datasets if ret is None else new_datasets[ret]\n",
        "\n",
        "    def remove(self, *datasets):\n",
        "        remove_keys = {k for d in datasets for k in d.keys}\n",
        "        new_keys = [k for k in self.keys if k not in remove_keys]\n",
        "        return self.change_keys(new_keys)\n",
        "\n",
        "    def keep_key_startswith(self, key_start):\n",
        "        new_keys = [k for k in self.keys if k.startswith(key_start)]\n",
        "        return self.change_keys(new_keys)\n",
        "\n",
        "    def mod_single(self, mod_func, descriptor, i, keep_key, inputs_only):\n",
        "        queries = {}\n",
        "        replies = {}\n",
        "        keys    = []\n",
        "        for k0 in self.keys:\n",
        "            desc = (('copy{i}' if mod_func is np.copy else mod_func.__name__) if descriptor is None else descriptor if isinstance(descriptor, str) else descriptor(self.queries[k0])).format(i=i)\n",
        "            func = lambda a, d: np.asarray(mod_func(a) if descriptor is None else mod_func(a, d)).tolist()\n",
        "            k1 = k0 if keep_key else f\"{k0}.{'I' if inputs_only else ''}{desc}\"\n",
        "            keys.append(k1)\n",
        "            queries[k1] = {m: [{t: (func(a, desc) if t=='input' or not inputs_only else a) for t, a in x.items()} for x in e] for m, e in self.queries[k0].items()}\n",
        "            if k0 in self.replies:\n",
        "                replies[k1] = [func(a, desc) for a in self.replies[k0]]\n",
        "        ret = self.__class__(queries=queries, replies=replies, keys=keys)\n",
        "        return ret\n",
        "\n",
        "    def mod(self, mod_func, descriptor=None, n=1, stack=None, keep=False, keep_key=False, shuffle=False, join=True, inputs_only=False):\n",
        "        assert not (keep and keep_key)\n",
        "        cur = self\n",
        "        ret = [cur.shuffled() if shuffle else cur] if keep else []\n",
        "        if stack is None: stack = mod_func.__name__.startswith('rot')\n",
        "        for i in range(n):\n",
        "            cur = (cur if stack else self).mod_single(mod_func, descriptor, i=i, keep_key=keep_key, inputs_only=inputs_only)\n",
        "            ret.append(cur.shuffled() if shuffle else cur)\n",
        "        return self.__class__.append(*ret) if join else ret\n",
        "\n",
        "    def get(self, key, formatter):\n",
        "        assert formatter.out2_token is None or key in self.replies\n",
        "        train = formatter.fmt_train(self.queries[key]['train'])\n",
        "        query = formatter.fmt_query(self.queries[key]['test'], i=len(self.queries[key]['train']))\n",
        "        reply = formatter.fmt_reply(self.replies[key], self.faulty.get(key)) if key in self.replies else ''\n",
        "        text = train+query+reply if reply else formatter.fmt_train(self.queries[key]['train'], last_is_challenge=True)\n",
        "        return dict(key=key, train=train, query=query, reply=reply, input=train+query, text=text)\n",
        "\n",
        "    def as_list(self, formatter):\n",
        "        return [self.get(key, formatter) for key in self.keys]\n",
        "\n",
        "    def as_dataset(self):\n",
        "        from datasets import Dataset\n",
        "        return Dataset.from_list([{'key': k, 'query': self.queries[k], 'reply': self.replies[k]} for k in self.keys])\n",
        "\n",
        "    def get_length(self, key, formatter, name, max_of_transposed=False):\n",
        "        if formatter is None:\n",
        "            if   name=='input': return sum(np.prod(np.shape(v)) for v3 in self.queries[key].values() for v2 in v3 for v in v2.values())\n",
        "            elif name=='reply': return sum(np.prod(np.shape(v)) for v in self.replies[key])\n",
        "            else: assert False\n",
        "        else:\n",
        "            datasets = [self]\n",
        "            if max_of_transposed:\n",
        "                if self.transposed_dataset is None: self.transposed_dataset = self.mod(np.transpose, keep=False, keep_key=True)\n",
        "                datasets.append(self.transposed_dataset)\n",
        "            return max(len(formatter.tokenizer(ds.get(key, formatter=formatter)[name])['input_ids']) for ds in datasets)\n",
        "\n",
        "    def get_lengths(self, formatter, name, max_of_transposed=False):\n",
        "        return {key: self.get_length(key, formatter=formatter, name=name, max_of_transposed=max_of_transposed) for key in self.keys}\n",
        "\n",
        "    def sorted_by_len(self, reverse=False, **kwargs):\n",
        "        new_keys = [key for _, key in sorted([(v, k) for k, v in self.get_lengths(**kwargs).items()], reverse=reverse)]\n",
        "        return self.change_keys(new_keys)\n",
        "\n",
        "    def filter_by_len(self, min_len=0, max_len=float('inf'), **kwargs):\n",
        "        new_keys = [k for k, v in self.get_lengths(**kwargs).items() if min_len<=v<=max_len]\n",
        "        return self.change_keys(new_keys)\n",
        "\n",
        "    def cut_to_query_count(self, max_count, from_end=False):\n",
        "        new_queries = {}\n",
        "        for k in self.keys:\n",
        "            new_queries[k] = q = self.queries[k]\n",
        "            while len(q['train'])>max_count: q['train'] = q['train'][:-1] if from_end else q['train'][1:]\n",
        "        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n",
        "\n",
        "    def cut_to_len(self, formatter, name, max_len, max_new_tokens='auto', from_end=False, quiet=False, **kwargs):\n",
        "        if max_new_tokens:\n",
        "            if max_new_tokens=='auto': max_new_tokens = formatter.max_new_tokens()\n",
        "            max_len_old, max_len = max_len, max_len - max_new_tokens\n",
        "            if not quiet: print(f'*** Reducing task size to max. {max_len_old} tokens ({max_len} input + {max_new_tokens} generated)...')\n",
        "        elif not quiet: print(f'*** Reducing task size to max. {max_len} tokens...')\n",
        "        temp_ds = self.change_keys(self.keys)\n",
        "        new_keys = []\n",
        "        new_queries = {}\n",
        "        new_replies = {}\n",
        "        for key in (self.keys if quiet else tqdm(self.keys, file=sys.stdout)):\n",
        "            reply = temp_ds.replies.get(key)\n",
        "            while max_len<temp_ds.get_length(key, formatter=formatter, name=name, **kwargs):\n",
        "                query = temp_ds.queries[key]\n",
        "                if not key.split('.')[-1].startswith('ex'): key = f\"{key}.ex{''.join(map(str, range(len(query['train']))))}\"\n",
        "                key_split = key.split('.')\n",
        "                assert key_split[-1].startswith('ex')\n",
        "                key = '.'.join(key_split[:-1] + [f'ex{key_split[-1][2:-1] if from_end else key_split[-1][3:]}'])\n",
        "                temp_ds.queries[key] = {k: ((v[:-1] if from_end else v[1:]) if k=='train' else v) for k, v in query.items()}\n",
        "                if reply is not None: temp_ds.replies[key] = reply\n",
        "            new_keys.append(key)\n",
        "            new_queries[key] = temp_ds.queries[key]\n",
        "            if reply is not None: new_replies[key] = reply\n",
        "        return self.__class__(keys=new_keys, queries=new_queries, replies=new_replies)\n",
        "\n",
        "    def shuffle_ex(self, perm=None, keep_max=None):\n",
        "        new_keys = []\n",
        "        new_queries = {}\n",
        "        new_replies = {}\n",
        "        for key in self.keys:\n",
        "            n = len(self.queries[key]['train'])\n",
        "            p = np.random.permutation(n) if perm is None else perm\n",
        "            if keep_max is not None: p = p[:keep_max]\n",
        "            new_key = f'{key}.ex' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n",
        "            new_keys.append(new_key)\n",
        "            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='train' else v) for k, v in self.queries[key].items()}\n",
        "            if key in self.replies: new_replies[new_key] = self.replies[key]\n",
        "        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n",
        "\n",
        "    def shuffle_rp(self, keep_max=None):\n",
        "        new_keys = []\n",
        "        new_queries = {}\n",
        "        new_replies = {}\n",
        "        for key in self.keys:\n",
        "            n = len(self.queries[key]['test'])\n",
        "            p = np.random.permutation(n)\n",
        "            if keep_max is not None: p = p[:keep_max]\n",
        "            new_key = f'{key}.rp' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n",
        "            new_keys.append(new_key)\n",
        "            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='test' else v) for k, v in self.queries[key].items()}\n",
        "            if key in self.replies: new_replies[new_key] = np.array(self.replies[key], dtype=object)[p].tolist()\n",
        "        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n",
        "\n",
        "    def append_to_keys(self, test):\n",
        "        return self.change_keys([f'{k}{text}' for k in self.keys])\n",
        "\n",
        "    def random_select(self, n):\n",
        "        keys = np.array(self.keys).reshape(n, -1).T\n",
        "        choice = np.random.randint(0, n, size=[len(keys)])\n",
        "        return self.change_keys(keys[np.arange(len(keys)), choice])\n",
        "\n",
        "    def augment(self, tp=False, rot=False, n=1, perm=None, perm_append=False, shfl_keys=False, shfl_ex=False, seed=None, quiet=False, inputs_only=False):\n",
        "        if not quiet: print(f\"*** Augment dataset{' (inputs only)' if inputs_only else ''}...\")\n",
        "        np.random.seed(seed)\n",
        "        d = self\n",
        "        if tp: d = d.mod(np.transpose, keep=True, inputs_only=inputs_only)\n",
        "        if tp=='rand': d = d.random_select(n=2)\n",
        "        if rot: d = d.mod(np.rot90, n=3, keep=True, inputs_only=inputs_only)\n",
        "        if rot=='rand': d = d.random_select(n=4)\n",
        "        if perm is None and n<=1: d = d.shuffled() if shfl_keys else d\n",
        "        else: d = d.mod(*([np.copy] if perm is None else globals()[f\"permute_{perm}\"]), n=n, shuffle=shfl_keys, keep=perm_append, inputs_only=inputs_only)\n",
        "        np.random.seed(seed)\n",
        "        if shfl_ex: d = d.shuffle_ex()\n",
        "        return d\n",
        "\n",
        "    def remove_replies(self):\n",
        "        return self.__class__(queries=self.queries, replies={}, keys=[k for k in self.keys])\n",
        "\n",
        "    def split_at_pos(self, pos, random_seed=None):\n",
        "        keys = self.keys\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            keys = np.random.permutation(keys)\n",
        "        if isinstance(pos, float): pos = int(pos * len(self.keys) + 0.5)\n",
        "        keys_split = [keys[:pos], keys[pos:]]\n",
        "        return tuple(self.change_keys(new_keys, keep_flags=True) for new_keys in keys_split)\n",
        "\n",
        "    def get_submission(self, results=None):\n",
        "        assert self.is_orig==True, 'Must be run on original dataset.'\n",
        "        submission = {k: [{f'attempt_{i+1}': [[0]] for i in range(2)} for _ in range(len(self.queries[k]['test']))] for k in self.keys}\n",
        "        if results is not None: self.fill_submission(results, submission)\n",
        "        return submission\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_submission(results, submission):\n",
        "        print(f'*** Generating submission for {len(results)} outputs...')\n",
        "        for k, v in results.items():\n",
        "            base_id, base_nr = k.split('_')\n",
        "            target_dict = submission[base_id][int(base_nr)]\n",
        "            for i, g in enumerate(v[:len(target_dict)]):\n",
        "                target_dict[f'attempt_{i+1}'] = g.tolist()\n",
        "\n",
        "    def validate_submission(self, submission):\n",
        "        assert self.is_orig==True, 'Must be run on original dataset.'\n",
        "        score = 0\n",
        "        for k, v in self.replies.items():\n",
        "            for i, r in enumerate(v):\n",
        "                for attempt in ['attempt_1', 'attempt_2']:\n",
        "                    if np.array_equal(r, submission[k][i][attempt]):\n",
        "                        score += 1 / len(v)\n",
        "                        break\n",
        "        return score\n",
        "def get_class_MyDataCollator(cache=[]):\n",
        "    if not cache:\n",
        "        from trl import DataCollatorForCompletionOnlyLM\n",
        "        class MyDataCollator(DataCollatorForCompletionOnlyLM):\n",
        "            def setup(self, out2_token_id=None, fault_token_id=None, fault_freq=0, sample_tries=8, mask_first_output=False):\n",
        "                self.out2_token_id = out2_token_id\n",
        "                self.fault_token_id = fault_token_id\n",
        "                self.fault_freq = fault_freq\n",
        "                self.sample_tries = sample_tries\n",
        "                self.mask_first_output = mask_first_output\n",
        "                return self\n",
        "\n",
        "            def torch_call(self, examples):\n",
        "                batch = super().torch_call(examples)\n",
        "                if self.out2_token_id is not None:\n",
        "                    assert not self.fault_freq\n",
        "                    for i in range(len(batch['input_ids'])):\n",
        "                        end_pos = ((batch['labels'][i] != -100              ).nonzero().max()).item() + 1\n",
        "                        mid_pos = ((batch['labels'][i] == self.out2_token_id).nonzero().max()).item() + 1\n",
        "                        beg_pos = mid_pos - (end_pos - mid_pos)\n",
        "                        batch['labels'][i][beg_pos:mid_pos] = batch['labels'][i][mid_pos:end_pos]\n",
        "                elif self.fault_freq:\n",
        "                    for i in range(len(batch['input_ids'])):\n",
        "                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n",
        "                        if not isinstance(self.fault_freq, float):\n",
        "                            eos_token_id = batch['labels'][i][end_pos - 1]\n",
        "                            num_examples = (batch['labels'][i] == eos_token_id).sum().item() - 1\n",
        "                            fault_freq = self.fault_freq[num_examples]\n",
        "                        else: fault_freq = self.fault_freq\n",
        "                        if random.random() < fault_freq:\n",
        "                            beg_pos = ((batch['labels'][i][:end_pos]==-100).nonzero().max()).item() + 1\n",
        "                            fault_pos = random.randint(beg_pos, end_pos-2)\n",
        "                            fault_tok = batch['labels'][i][fault_pos].item()\n",
        "                            for t in range(self.sample_tries):\n",
        "                                new_tok = batch['labels'][i][random.randint(beg_pos, end_pos-2)].item()\n",
        "                                if fault_tok!=new_tok:\n",
        "                                    batch['input_ids'][i][fault_pos] = new_tok\n",
        "                                    batch['labels'][i][fault_pos+1:end_pos] = self.fault_token_id\n",
        "                                    break\n",
        "                for i in range(len(batch['labels'])):\n",
        "                    for _ in range(self.mask_first_output):\n",
        "                        beg_pos = ((batch['labels'][i] != -100).nonzero().min()).item()\n",
        "                        mid_pos = ((batch['labels'][i][beg_pos:] == -100).nonzero().min()).item() + beg_pos\n",
        "                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n",
        "                        if mid_pos<end_pos: batch['labels'][i][beg_pos:mid_pos] = -100\n",
        "                return batch\n",
        "        cache.append(MyDataCollator)\n",
        "    return cache[0]\n",
        "\n",
        "class ArcFormatter(object):\n",
        "    def __init__(self, inp_prefix, out_prefix, arr_sep, out2_use=False, out2_token=None, arr_beg='', arr_end='', pretext='', pre_out=None, exa_sep='', exa_end='', qry_prefix=None, rpl_prefix=None, rpl_sep=None, dec_sep=None, min_wid=0, min_pad='', pretext_corpus_split='', masking=0, tokenizer=None, collator_kwargs={}, repeat_input_aug=None, repeat_input_pre=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inp_prefix = inp_prefix\n",
        "        self.out_prefix = out_prefix\n",
        "        self.out2_token = out2_token\n",
        "        self.out2_use = out2_use\n",
        "        assert not out2_use or out2_token is not None\n",
        "        assert not out2_use or masking in [1, 2]\n",
        "        assert masking!=2 or out2_use or rpl_prefix is not None\n",
        "        self.qry_prefix = qry_prefix if qry_prefix is not None else inp_prefix\n",
        "        self.rpl_prefix = rpl_prefix if rpl_prefix is not None else out_prefix\n",
        "        self.rpl_sep = rpl_sep if rpl_sep is not None else self.rpl_prefix\n",
        "        self.arr_sep = arr_sep\n",
        "        self.arr_beg = arr_beg\n",
        "        self.arr_end = arr_end\n",
        "        self.pretext = pretext\n",
        "        self.pre_out = pre_out\n",
        "        self.pre_out_empty = ['']*99\n",
        "        self.pretext_corpus_split = pretext_corpus_split\n",
        "        self.exa_sep = exa_sep\n",
        "        self.exa_end = exa_end\n",
        "        self.dec_sep = arr_sep if dec_sep is None else dec_sep\n",
        "        self.min_wid = min_wid\n",
        "        self.min_pad = min_pad\n",
        "        self.masking = masking\n",
        "        self.collator_kwargs = collator_kwargs\n",
        "        self.repeat_input_aug = repeat_input_aug\n",
        "        self.repeat_input_pre = repeat_input_pre\n",
        "\n",
        "    def fmt_array(self, array):\n",
        "        return self.arr_beg + self.arr_sep.join(str(row).replace(' ', '').replace(',', '').replace('[', '').replace(']', '')+self.min_pad*max(0, self.min_wid-len(row)) for row in array) + self.arr_end\n",
        "\n",
        "    def get_pre_out(self, pretext_split):\n",
        "        if self.pre_out is None: return self.pre_out_empty\n",
        "        if pretext_split: return [self.pretext_corpus_split.join(list(p) + ['']) for p in self.pre_out]\n",
        "        return self.pre_out\n",
        "\n",
        "    def fmt_train(self, train, last_is_challenge=False, pretext_split=False):\n",
        "        po = self.get_pre_out(pretext_split=pretext_split)\n",
        "        ex = [(f\"{self.fmt_query([x], i, pretext_split=pretext_split)}{self.fmt_reply([x['output']])}\" if last_is_challenge and i+1==len(train) else\n",
        "               f\"{self.inp_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.out_prefix}{self.fmt_array(x['output'])}\") for i, x in enumerate(train)]\n",
        "        pre = self.pretext_corpus_split.join(list(self.pretext)+['']) if pretext_split else self.pretext\n",
        "        end = '' if last_is_challenge else (self.exa_end + self.tokenizer.eos_token)\n",
        "        return pre + (self.exa_end + self.tokenizer.eos_token + self.exa_sep).join(ex) + end\n",
        "\n",
        "    def fmt_query(self, query, i, pretext_split=False):\n",
        "        po = self.get_pre_out(pretext_split=pretext_split)\n",
        "        return ''.join(f\"{self.qry_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.rpl_prefix}\" for x in query[:1])\n",
        "\n",
        "    def repeat_input(self, x, no_aug=False):\n",
        "        if self.repeat_input_aug is None: return ''\n",
        "        return f\"{self.repeat_input_pre}{self.fmt_array(((lambda x: x) if no_aug else self.repeat_input_aug)(x['input']))}\"\n",
        "\n",
        "    def fmt_reply(self, reply, fault=None):\n",
        "        ids = self.fmt_array(reply[0]) + self.exa_end + self.tokenizer.eos_token\n",
        "        if self.out2_use:\n",
        "            if fault is None: fault = reply\n",
        "            ids = self.fmt_array(fault[0]) + self.exa_end + self.out2_token + ids\n",
        "        return ids\n",
        "\n",
        "    def quick_test(self, decoded, done):\n",
        "        sp = decoded.split(self.tokenizer.eos_token)[0].split(self.dec_sep)\n",
        "        sl = len(sp[0])\n",
        "        is_prefix = sl>0 and len(sp[-1])<=sl and (len(sp)==1 or len(sp[-2])==sl) and all(x.isdigit() for x in sp[-1])\n",
        "        return is_prefix and (not done or len(sp[-1])==0 or len(sp[-1])==sl)\n",
        "\n",
        "    @staticmethod\n",
        "    def is_valid_solution(guess):\n",
        "        return isinstance(guess, np.ndarray) and guess.ndim == 2 and all(0 < x <= 30 for x in guess.shape)\n",
        "\n",
        "    def max_new_tokens(self, safety_margin=1):\n",
        "        max_sized_reply = np.zeros([30, 30], dtype=int)\n",
        "        tokenized = self.tokenizer(self.fmt_reply([max_sized_reply]))['input_ids']\n",
        "        max_new_tokens = len(tokenized)\n",
        "        if tokenized[0]==self.tokenizer.bos_token_id: max_new_tokens -= 1\n",
        "        return max_new_tokens + safety_margin\n",
        "\n",
        "    def de_tokenize(self, tokens, scores=None):\n",
        "        import torch\n",
        "        tokens_cut = cut_at_token(tokens, self.tokenizer.eos_token_id)\n",
        "        de_tokenized = self.tokenizer.batch_decode([tokens_cut])[0]\n",
        "        score_val = None\n",
        "        if scores is not None:\n",
        "            tokens_with_eos = tokens[:len(tokens_cut)+1]\n",
        "            score_val = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1).numpy().copy()[np.arange(len(tokens_with_eos)), tokens_with_eos].sum()\n",
        "            number_token_ids = [self.tokenizer.vocab[k] for k in map(str, range(10))]\n",
        "            fault_token_id = self.collator_kwargs.get('fault_token_id')\n",
        "            if fault_token_id is not None: number_token_ids.append(fault_token_id)\n",
        "            number_token_ids = np.array(number_token_ids)\n",
        "            number_positions = (tokens_cut[..., np.newaxis] == number_token_ids).any(-1)\n",
        "            scores = scores[:len(tokens_cut), number_token_ids][number_positions]\n",
        "            scores = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1)[:, :10].numpy().copy()\n",
        "        return max(len(tokens)+1, len(tokens_cut)), score_val, de_tokenized, scores\n",
        "\n",
        "    def decode_to_array_single(self, text, score=None, limit_rows=30):\n",
        "        try:\n",
        "            by_rows = [row for row in [[int(x) for x in line if x.isdigit()] for line in text.split(self.dec_sep)] if len(row)]\n",
        "            if limit_rows and len(by_rows) > limit_rows:\n",
        "                by_rows = by_rows[:limit_rows]\n",
        "                limited = True\n",
        "            else: limited = False\n",
        "            decoded = np.array(by_rows, dtype=int)\n",
        "            if self.is_valid_solution(decoded):\n",
        "                try:\n",
        "                    assert score is not None\n",
        "                    decoded_flat = decoded.ravel()\n",
        "                    if limited: score = score[:len(decoded_flat)]\n",
        "                    score_all = score.reshape(decoded.shape + score.shape[1:])\n",
        "                    score_result = score[range(len(decoded_flat)), decoded_flat]\n",
        "                    score_reshaped = score_result.reshape(decoded.shape)\n",
        "                    score_cum_reshaped = score_result.cumsum().reshape(score_reshaped.shape)\n",
        "                    score_all_cum = score_cum_reshaped[..., np.newaxis] - score_reshaped[..., np.newaxis] + score_all\n",
        "                except: score_reshaped = score_cum_reshaped = np.full(decoded.shape, -float('inf'))\n",
        "                return {'output': decoded, 'score': score_reshaped, 'score_cum': score_cum_reshaped, 'score_all': score_all, 'score_all_cum': score_all_cum}\n",
        "        except: pass\n",
        "        return {}\n",
        "\n",
        "    def decode_to_array(self, text, score=None, limit_rows=30):\n",
        "        if not self.out2_use: text, score = [text], [score]\n",
        "        else:\n",
        "            text = text.split(self.out2_token)\n",
        "            if score is None: score = [None]*len(text)\n",
        "            else:\n",
        "                lengths = np.cumsum([len(list(filter(str.isdigit, t))) for t in text])\n",
        "                score = [score[s:e] for s, e in zip([0]+lengths[:-1].tolist(), lengths)]\n",
        "        return [self.decode_to_array_single(t, s) for t, s in zip(text, score)]\n",
        "\n",
        "    def get_corpus(self):\n",
        "        try:\n",
        "            old_min_wid, self.min_wid = self.min_wid, min(self.min_wid, 2)\n",
        "            return self.fmt_train([{'input': [[i] for i in range(10)], 'output': [[i] for i in range(10)]}]*3, last_is_challenge=True, pretext_split=True)\n",
        "        finally: self.min_wid = old_min_wid\n",
        "\n",
        "    def get_data_collator(self):\n",
        "        if not self.masking: return None\n",
        "        from transformers import DataCollatorForLanguageModeling\n",
        "        collator_params = dict(tokenizer=self.tokenizer, mlm=False)\n",
        "        pass_out2_token = self.tokenizer.vocab[self.out2_token] if self.out2_use and self.masking==1 else None\n",
        "        if self.masking:\n",
        "            assert not self.collator_kwargs.get('mask_first_output') or self.masking==1\n",
        "            data_collator = get_class_MyDataCollator()(\n",
        "                **collator_params,\n",
        "                instruction_template=[self.inp_prefix, self.tokenizer.bos_token][self.masking - 1],\n",
        "                response_template=[self.out_prefix, (self.out2_token if self.out2_use else self.rpl_sep)][self.masking - 1],\n",
        "            ).setup(out2_token_id=pass_out2_token, **self.collator_kwargs)\n",
        "        else:\n",
        "            assert not self.collator_kwargs, 'only supported with masking on'\n",
        "            data_collator = DataCollatorForLanguageModeling(**collator_params)\n",
        "        return data_collator\n",
        "\n",
        "    def get_output_token_ids(self):\n",
        "        assert not self.out2_use\n",
        "        num_tokens = [self.tokenizer.vocab[str(i)] for i in range(10)]\n",
        "        sep_tokens = [tok for txt in [self.arr_beg, self.arr_sep, self.arr_end, self.exa_sep] if txt for tok in self.tokenizer(txt)['input_ids'][1:]]\n",
        "        sep_tokens.append(self.tokenizer.eos_token_id)\n",
        "        return num_tokens + sorted(set(sep_tokens))\n",
        "\n",
        "ArcFormatter_pretext2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pretext_corpus_split='\\n', **kwargs)\n",
        "ArcFormatter_pretext3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pretext_corpus_split='\\n', **kwargs)\n",
        "ArcFormatter_premix_2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\n",
        "ArcFormatter_premix_3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\n",
        "\n",
        "available_formatters = dict(\n",
        "    ArcFormatter_pretext2=ArcFormatter_pretext2,\n",
        "    ArcFormatter_pretext3=ArcFormatter_pretext3,\n",
        "    ArcFormatter_premix_2=ArcFormatter_premix_2,\n",
        "    ArcFormatter_premix_3=ArcFormatter_premix_3,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.654055Z",
          "iopub.execute_input": "2024-11-07T17:26:31.654358Z",
          "iopub.status.idle": "2024-11-07T17:26:31.689Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.654322Z",
          "shell.execute_reply": "2024-11-07T17:26:31.688029Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5WLvp5IMBWm",
        "outputId": "9f8fd583-147b-4174-de99-e0aeb90b1962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing arc_loader.py\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile selection.py\n",
        "import numpy as np\n",
        "\n",
        "def hashable(guess):\n",
        "    return tuple(map(tuple, guess))\n",
        "\n",
        "def make_unique(guess_list, indices=None):\n",
        "    used = set()\n",
        "    out = []\n",
        "    out_ind = []\n",
        "    for i, g in enumerate(guess_list):\n",
        "        h = hashable(g)\n",
        "        if h not in used:\n",
        "            used.add(h)\n",
        "            out.append(np.array(g))\n",
        "            if indices is not None: out_ind.append(indices[i])\n",
        "    return out if indices is None else (out, out_ind)\n",
        "\n",
        "def first_only(guesses):\n",
        "    return [g['output'] for g in guesses.values()][:1]\n",
        "\n",
        "def keep_order(guesses):\n",
        "    return [g['output'] for g in guesses.values()]\n",
        "\n",
        "def keep_order_unique(guesses):\n",
        "    return make_unique(keep_order(guesses))\n",
        "\n",
        "def get_best_shape_by_score(guess_list, getter, once_per_result=True):\n",
        "    seen_outputs = set()\n",
        "    shape_scores = {}\n",
        "    for i, g in enumerate(guess_list):\n",
        "        shape = tuple(g['output'].shape)\n",
        "        scores = shape_scores[shape] = shape_scores.get(shape, [[], []])\n",
        "        scores[1].append(i)\n",
        "        h = hashable(g['output'])\n",
        "        if h in seen_outputs: continue\n",
        "        if once_per_result: seen_outputs.add(h)\n",
        "        scores[0].append(g)\n",
        "    shape_scores = [(getter(scores), shape, indices) for shape, (scores, indices) in shape_scores.items()]\n",
        "    shape_scores = sorted(shape_scores, key=(lambda x: x[0]), reverse=True)\n",
        "    return shape_scores[0]\n",
        "\n",
        "def score_sum(guesses, getter, shape_getter=None, prefer_common_shape=True):\n",
        "    if shape_getter is None: shape_getter = getter\n",
        "    guess_list = list(guesses.values())\n",
        "    common_shape_indices = set(get_best_shape_by_score(guess_list, shape_getter)[2]) if prefer_common_shape else []\n",
        "    scores = {}\n",
        "    for i, g in enumerate(guess_list):\n",
        "        h = hashable(g['output'])\n",
        "        x = scores[h] = scores.get(h, [i in common_shape_indices, [], g['output']])\n",
        "        x[1].append(g)\n",
        "    scores = [(cs, getter(sc), o) for cs, sc, o in scores.values()]\n",
        "    scores = sorted(scores, key=(lambda x: x[:2]), reverse=True)\n",
        "    ordered_outputs = [x[-1] for x in scores]\n",
        "    return ordered_outputs\n",
        "\n",
        "getter_all_probsum = lambda guesses: sum(np.exp(g['score_val']) for g in guesses)\n",
        "def score_all_probsum(guesses): return score_sum(guesses, getter_all_probsum)\n",
        "\n",
        "def getter_full_probmul(p):\n",
        "    def _getter(guesses, baseline=p):\n",
        "        inf_score = sum([g['score_val']+baseline for g in guesses])\n",
        "        aug_score = np.mean([sum(s+baseline for s in g['score_multi_nl']) for g in guesses])\n",
        "        return inf_score + aug_score\n",
        "    return _getter\n",
        "\n",
        "def score_full_probmul_3(guesses): return score_sum(guesses, getter_full_probmul(3), prefer_common_shape=False)\n",
        "\n",
        "selection_algorithms = [\n",
        "    first_only,\n",
        "    keep_order,\n",
        "    keep_order_unique,\n",
        "    score_all_probsum,\n",
        "    score_full_probmul_3,\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.72151Z",
          "iopub.execute_input": "2024-11-07T17:26:31.721962Z",
          "iopub.status.idle": "2024-11-07T17:26:31.732174Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.721917Z",
          "shell.execute_reply": "2024-11-07T17:26:31.731273Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM_PBbbAMBWn",
        "outputId": "b220466e-f805-4301-cfdc-502b47dddca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing selection.py\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile async_tools.py\n",
        "import sys\n",
        "import asyncio\n",
        "\n",
        "async def stream_reader(stream, id, to):\n",
        "    id = '' if id is None else f'{id}. '\n",
        "    data = b''\n",
        "    while True:\n",
        "        read = await stream.read(n=4096)\n",
        "        if not read: break\n",
        "        if to is not None:\n",
        "            *complete_lines, data = (data + read + b'X').splitlines()\n",
        "            data = data[:-1]\n",
        "            for line in complete_lines:\n",
        "                line = line.rstrip()\n",
        "                if line: print(f\"{id}{line.decode('utf-8')}\", file=to, end='\\n', flush=True)\n",
        "\n",
        "async def wait_for_subprocess(subprocess, print_output=False, id=None):\n",
        "    await asyncio.gather(\n",
        "            stream_reader(subprocess.stdout, id, (sys.stdout if print_output else None)),\n",
        "            stream_reader(subprocess.stderr, id, (sys.stderr if print_output else None)),\n",
        "        )\n",
        "    return await subprocess.wait()\n",
        "\n",
        "async def wait_for_subprocesses(*processes, print_output=False):\n",
        "    return await asyncio.gather(*[wait_for_subprocess(p, print_output=print_output, id=i if len(processes)>1 else None) for i, p in enumerate(processes)])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.734733Z",
          "iopub.execute_input": "2024-11-07T17:26:31.735109Z",
          "iopub.status.idle": "2024-11-07T17:26:31.742661Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.735076Z",
          "shell.execute_reply": "2024-11-07T17:26:31.74183Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8KT36HRMBWo",
        "outputId": "12c9ab32-8219-4edf-90c9-b4f09f164627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing async_tools.py\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile common_stuff.py\n",
        "# common configuration for training and evaluation\n",
        "from arc_loader import *\n",
        "from model_runner import *\n",
        "from selection import *\n",
        "from async_tools import *\n",
        "import time\n",
        "\n",
        "# paths\n",
        "tmp_dir = '/kaggle/temp'\n",
        "arc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n",
        "arc_solutions_file = '/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json'\n",
        "model_temp_storage = os.path.join(tmp_dir, 'finetuned_model')\n",
        "infer_temp_storage = os.path.join(tmp_dir, 'inference_outputs')\n",
        "score_temp_storage = os.path.join(tmp_dir, 'inference_scoring')\n",
        "\n",
        "# load datasets\n",
        "arc_test_set = ArcDataset.from_file(arc_challenge_file)\n",
        "if arc_test_set.is_fake: arc_test_set.load_replies(arc_solutions_file)\n",
        "#arc_test_set.is_fake = False  # force full run\n",
        "#arc_train_set = ArcDataset.from_file('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
        "\n",
        "# models\n",
        "base_model, MyFormatter, perm_aug, max_seq_length_train, mask_first = '/kaggle/input/wb55l_nemomini_fulleval/transformers/default/1', ArcFormatter_premix_3, 'rnd_all', 4224, 0\n",
        "\n",
        "# training & inference\n",
        "train_epochs = 4\n",
        "multi_gpu_train = True\n",
        "multi_gpu_random_split = True\n",
        "max_seq_length_infer = 8192\n",
        "prime_on_single_task = False\n",
        "infer_params = dict(min_prob=0.17, store=infer_temp_storage, use_turbo=True)\n",
        "\n",
        "# scoring\n",
        "use_aug_score = True\n",
        "aug_score_params = dict(tp=True, rot=True, perm=perm_aug, shfl_ex=True, make_unique=True, max_len=max_seq_length_infer)\n",
        "submission_select_algo = score_full_probmul_3 if use_aug_score else score_all_probsum\n",
        "\n",
        "def prepare_run(model_path, load_lora=None, train=False, gpu=None, **kwargs):\n",
        "    if gpu is not None:\n",
        "        os.environ[\"CUDA_DEVICE_ORDER\"   ] = \"PCI_BUS_ID\"\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
        "\n",
        "    model, tokenizer, formatter = prepare_model(  # base model configuration\n",
        "        model=model_path,\n",
        "        local_files_only=True,\n",
        "        mode='unsloth_4bit',\n",
        "        #shrink_embedding=8000,\n",
        "        max_seq_length=max_seq_length_train,\n",
        "        formatter=MyFormatter,\n",
        "        peft=([dict(\n",
        "            r=64,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'embed_tokens', 'lm_head'],\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "            bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
        "            use_gradient_checkpointing=True,  # True or \"unsloth\" for very long context\n",
        "            random_state=42,\n",
        "            use_rslora=True,  # We support rank stabilized LoRA\n",
        "            loftq_config=None,  # And LoftQ\n",
        "        )] if train or load_lora else []) + ([load_lora] if load_lora else []),\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    if train and mask_first: formatter.collator_kwargs.update(mask_first_output=mask_first)\n",
        "\n",
        "    return model, formatter\n",
        "\n",
        "def prepare_dataset(formatter, train, gpu=None):\n",
        "    ds = arc_test_set\n",
        "    if multi_gpu_train and gpu is not None:\n",
        "        if multi_gpu_random_split:\n",
        "            ds = ds.shuffled(seed=123)\n",
        "            ds = ds.split_at_pos(len(ds.keys)//2)[gpu]\n",
        "        else:\n",
        "            ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n",
        "            assignment = ([0,1,1,0]*ds.length())[:ds.length()][::-1]\n",
        "            ds = ds.change_keys((np.array(ds.keys)[np.array(assignment)==gpu]).tolist())\n",
        "    if train:\n",
        "        ds = ds.remove_replies()\n",
        "        ds = ds.augment(tp=True, rot=True, perm=perm_aug, n=(2 if arc_test_set.is_fake else train_epochs), shfl_ex=True, shfl_keys=True)\n",
        "        ds = ds.cut_to_len(formatter=formatter, name='text', max_len=max_seq_length_train, max_new_tokens=0)\n",
        "        if arc_test_set.is_fake: ds = ds.sorted_by_len(formatter=formatter, name='text', reverse=True)\n",
        "    else:\n",
        "        ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n",
        "        ds = ds.split_multi_replies()\n",
        "        ds = ds.augment(tp=True, rot=True, n=2, seed=42, perm=perm_aug, shfl_ex=True).interleave(ds.length())\n",
        "        ds = ds.cut_to_len(formatter=formatter, name='input', max_len=max_seq_length_infer)\n",
        "        if arc_test_set.is_fake: ds.keys = ds.keys[:128] #ds.keys[::-1][::5][::-1]\n",
        "    return ds\n",
        "\n",
        "def start_training(gpu):\n",
        "    try:\n",
        "        storage_path = f'{model_temp_storage}_gpu{gpu}'\n",
        "        if (gpu==0 or multi_gpu_train) and not os.path.exists(storage_path):\n",
        "            with RemapCudaOOM():\n",
        "                model, formatter = prepare_run(base_model, train=True, gpu=gpu)\n",
        "                dataset = prepare_dataset(formatter, train=True, gpu=gpu if multi_gpu_train else None)\n",
        "                model, trainer_stats = training_run(\n",
        "                    model, formatter, dataset, store=storage_path,\n",
        "                    max_seq_length=max_seq_length_train,\n",
        "                    grad_acc_fix=False,\n",
        "                    train_args=dict(\n",
        "                        per_device_train_batch_size=2,\n",
        "                        gradient_accumulation_steps=2,\n",
        "                        warmup_steps=100,\n",
        "                        num_train_epochs=1,\n",
        "                        #max_steps=20 if arc_test_set.is_fake else -1,\n",
        "                        max_steps=20 if arc_test_set.is_fake else 40, #ins 20250329\n",
        "                        learning_rate=1e-4,\n",
        "                        embedding_learning_rate=1e-5,\n",
        "                        logging_steps=10,\n",
        "                        optim=\"adamw_8bit\",\n",
        "                        weight_decay=0.01,  # 0.01,\n",
        "                        lr_scheduler_type='cosine',  # \"linear\", \"cosine\",\n",
        "                        seed=42,\n",
        "                        output_dir=os.path.join(tmp_dir, 'checkpoints'),\n",
        "                        save_strategy=\"no\",\n",
        "                        report_to='none',\n",
        "                    ),\n",
        "                )\n",
        "                mem_info()\n",
        "    finally: os.makedirs(f'{storage_path}_done', exist_ok=True)\n",
        "\n",
        "def start_inference(gpu):\n",
        "    storage_path = f'{model_temp_storage}_gpu{gpu if multi_gpu_train else 0}'\n",
        "    while not os.path.exists(f'{storage_path}_done'): time.sleep(15)\n",
        "    with RemapCudaOOM():\n",
        "        model, formatter = prepare_run(storage_path, gpu=gpu)\n",
        "        dataset = prepare_dataset(formatter, train=False, gpu=gpu)\n",
        "        retrainer = None if not prime_on_single_task else Retrainer(\n",
        "            n=32,\n",
        "            aug_opts=dict(perm=perm_aug, shfl_ex=True),\n",
        "            reload_state_dict=get_and_fix_peft_weights(storage_path),\n",
        "            formatter=formatter,\n",
        "            max_seq_length=max_seq_length_infer,\n",
        "            grad_acc_fix=False,\n",
        "            train_args=dict(\n",
        "                per_device_train_batch_size=2,\n",
        "                gradient_accumulation_steps=2,\n",
        "                warmup_steps=4,\n",
        "                num_train_epochs=1,\n",
        "                learning_rate=1e-4,\n",
        "                embedding_learning_rate=0,\n",
        "                max_steps=20 , #ins 20250329\n",
        "                logging_steps=8,\n",
        "                optim=\"adamw_8bit\",\n",
        "                weight_decay=0.00,  # 0.01,\n",
        "                lr_scheduler_type='constant',  # \"linear\", \"cosine\",\n",
        "                seed=42,\n",
        "                output_dir='tmp_output',\n",
        "                save_strategy='no',\n",
        "                report_to='none',\n",
        "            ),\n",
        "        )\n",
        "        decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, prob_baseline=0.05)\n",
        "        inference_run_v2(model, formatter, dataset, decoder, retrain=retrainer, **infer_params)\n",
        "        if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
        "        mem_info()\n",
        "\n",
        "class RemapCudaOOM:\n",
        "    def __enter__(self): pass\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        oom_errors = [\"CUDA out of memory\", \"Make sure you have enough GPU RAM\", \"does not fit any GPU's remaining memory\"]\n",
        "        if exc_value and any(x in str(exc_value) for x in oom_errors):\n",
        "            with open('submission.json', 'w') as f: f.write('cause submission scoring error')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.744087Z",
          "iopub.execute_input": "2024-11-07T17:26:31.744479Z",
          "iopub.status.idle": "2024-11-07T17:26:31.758305Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.744433Z",
          "shell.execute_reply": "2024-11-07T17:26:31.75721Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cof2uUWMBWo",
        "outputId": "8233ddea-a878-4dd8-dc2d-ed02d81fe105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing common_stuff.py\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "StNyGafUYDLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from common_stuff import *\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "if not os.path.exists(os.path.join(tmp_dir, 'unsloth_installed')):  # unsloth offline install - https://stackoverflow.com/a/51646354\n",
        "    !pip uninstall --yes torch accelerate\n",
        "    !pip install --no-index --find-links=/kaggle/input/unsloth-2024-9-post4/wheelhouse unsloth\n",
        "    #!pip uninstall --yes accelerate fastai torch torchaudio transformers\n",
        "    #!pip install --no-index --find-links=/kaggle/input/unsloth-2024-10-7/wheelhouse unsloth  # do not use grad_acc_fix - trains very slow\n",
        "    #!sed -i 's/if ((post_check - pre_check) >= 1).sum() > 1:/if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\n",
        "    # fix delay bug in get_statistics()\n",
        "    !sed -i 's/^def get_statistics():/def get_statistics():\\n if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py\n",
        "    # fix faulty unsloth multi-gpu detection\n",
        "    !sed -i \"s/raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')/pass/g\" /opt/conda/lib/python3.10/site-packages/unsloth/tokenizer_utils.py /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py /opt/conda/lib/python3.10/site-packages/unsloth/models/vision.py\n",
        "    os.makedirs(os.path.join(tmp_dir, 'unsloth_installed'), exist_ok=True)\n",
        "    print('Unsloth installed & patched.')\n",
        "\n",
        "for gpu in [0, 1]:\n",
        "    signal_path = f'{model_temp_storage}_gpu{gpu}_done'\n",
        "    if os.path.exists(signal_path): os.rmdir(signal_path)\n",
        "\n",
        "if arc_test_set.is_fake:  # cleanup? (for debugging)\n",
        "    #!rm -R /kaggle/temp/finetuned_model*\n",
        "    #!rm -R /kaggle/temp/inference_outputs\n",
        "    #!rm -R /kaggle/temp/inference_scoring\n",
        "    #!ls /kaggle/temp\n",
        "    pass"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.759402Z",
          "iopub.execute_input": "2024-11-07T17:26:31.759733Z",
          "iopub.status.idle": "2024-11-07T17:26:31.843157Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.759664Z",
          "shell.execute_reply": "2024-11-07T17:26:31.842226Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "dlFiPVrAMBWo",
        "outputId": "dc1a100b-f516-4244-fc07-e4659820d420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Load challanges from '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-301173220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon_stuff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"WANDB_DISABLED\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unsloth_installed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# unsloth offline install - https://stackoverflow.com/a/51646354\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/common_stuff.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0marc_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArcDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marc_challenge_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marc_test_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fake\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marc_test_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_replies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marc_solutions_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#arc_test_set.is_fake = False  # force full run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/arc_loader.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, queries_file)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"*** Load challanges from '{queries_file}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_IS_COMPETITION_RERUN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Real submit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "%%python --bg --proc train_proc0\n",
        "from common_stuff import *\n",
        "start_training(gpu=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.844451Z",
          "iopub.execute_input": "2024-11-07T17:26:31.845103Z",
          "iopub.status.idle": "2024-11-07T17:26:31.858063Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.845056Z",
          "shell.execute_reply": "2024-11-07T17:26:31.857173Z"
        },
        "trusted": true,
        "id": "d-Ws5W5mMBWp"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "%%python --bg --proc train_proc1\n",
        "from common_stuff import *\n",
        "start_training(gpu=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.859188Z",
          "iopub.execute_input": "2024-11-07T17:26:31.859688Z",
          "iopub.status.idle": "2024-11-07T17:26:31.866834Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.859643Z",
          "shell.execute_reply": "2024-11-07T17:26:31.865796Z"
        },
        "trusted": true,
        "id": "7Z4fmUIgMBWp"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "%%python --bg --proc infer_proc0\n",
        "from common_stuff import *\n",
        "start_inference(gpu=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.867803Z",
          "iopub.execute_input": "2024-11-07T17:26:31.869152Z",
          "iopub.status.idle": "2024-11-07T17:26:31.878349Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.869105Z",
          "shell.execute_reply": "2024-11-07T17:26:31.87679Z"
        },
        "trusted": true,
        "id": "_gngY6BUMBWp"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "%%python --bg --proc infer_proc1\n",
        "from common_stuff import *\n",
        "start_inference(gpu=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.879645Z",
          "iopub.execute_input": "2024-11-07T17:26:31.88017Z",
          "iopub.status.idle": "2024-11-07T17:26:31.891099Z",
          "shell.execute_reply.started": "2024-11-07T17:26:31.88012Z",
          "shell.execute_reply": "2024-11-07T17:26:31.890006Z"
        },
        "trusted": true,
        "id": "piFTJrKZMBWp"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "proc_exit_codes = await wait_for_subprocesses(train_proc0, train_proc1, infer_proc0, infer_proc1, print_output=True or arc_test_set.is_fake)\n",
        "print(f'*** Subprocesses exit codes: {proc_exit_codes}')\n",
        "assert all(x==0 for x in proc_exit_codes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-07T17:26:31.892129Z",
          "iopub.execute_input": "2024-11-07T17:26:31.892501Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "zshITCfoMBWp",
        "outputId": "4c40d138-02c1-47d7-8487-b86affd6a2f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wait_for_subprocesses' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-901129334.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproc_exit_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mwait_for_subprocesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_proc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_proc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_proc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_proc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marc_test_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'*** Subprocesses exit codes: {proc_exit_codes}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc_exit_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wait_for_subprocesses' is not defined"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# write submission\n",
        "from common_stuff import *\n",
        "with RemapCudaOOM():\n",
        "    model, formatter, dataset = None, MyFormatter(), None\n",
        "    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, frac_score=True).from_store(infer_params['store'])\n",
        "    if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
        "    submission = arc_test_set.get_submission(decoder.run_selection_algo(submission_select_algo))\n",
        "    with open('submission.json', 'w') as f: json.dump(submission, f)\n",
        "    if arc_test_set.is_fake:\n",
        "        decoder.benchmark_selection_algos(selection_algorithms)\n",
        "        with open('submission.json') as f: reload_submission = json.load(f)\n",
        "        print('*** Reload score:', arc_test_set.validate_submission(reload_submission))"
      ],
      "metadata": {
        "trusted": true,
        "id": "44VR1Sd-MBWp"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}